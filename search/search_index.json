{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Inlets documentation \u00b6 Inlets reinvents the concept of a tunnel for a Cloud Native world. You can visit the inlets homepage at https://inlets.dev/ Note Welcome to the new inlets documentation site launched Nov 2021 . This rewrite aims to be less verbose, to connect you with practical task-driven examples and to make it easier to find what you need. If you have comments, questions or suggestions, please let us know on GitHub . With inlets you are in control of your data, unlike with a SaaS tunnel where shared servers mean your data may be at risk. You can use inlets for local development and in your production environment. It works just as well on bare-metal as in VMs, containers and Kubernetes clusters. inlets is not just compatible with tricky networks and Cloud Native architecture, it was purpose-built for them. Common use-cases include: Exposing local endpoints on the Internet Self-hosting from a homelab or on-premises datacenter Deploying and monitoring apps across multiple locations Remote customer support What is your strategy for connecting existing applications to the public cloud? Read: The Simple Way To Connect Existing Apps to Public Cloud . How does it work? \u00b6 Inlets tunnels connect to each other over a secure websocket with TLS encryption. Over that private connection, you can then tunnel HTTPS or TCP traffic to computers in another network or to the Internet. One of the most common use-cases is to expose a local HTTP endpoint on the Internet via a HTTPS tunnel. You may be working with webhooks, integrating with OAuth, sharing a draft of a blog post or integrating with a partner's API. After deploying an inlets HTTPS server on a public cloud VM, you can then connect the client and access it. There is more that inlets can do for you than exposing local endpoints. inlets also supports local forwarding and can be used to replace more cumbersome services like SSH, complex VPNs or expensive direct connect uplinks. Read more in the: the inlets FAQ . Getting started \u00b6 These guides walk you through a specific use-case with inlets. If you have questions or cannot find what you need, there are options for connecting with the community at the end of this page. Inlets can tunnel either HTTP or TCP traffic: HTTP (L7) tunnels can be used to connect one or more HTTP endpoints from one network to another. A single tunnel can expose multiple websites or hosts, including LoadBalancing and multiple clients to one server. TCP (L4) tunnels can be used to connect TCP services such as a database, a reverse proxy, RDP, Kubernetes or SSH to the Internet. A single tunnel can expose multiple ports on an exit-server and load balance between clients Downloading inlets \u00b6 inlets is available for Windows, MacOS (including M1) and Linux (including ARM): Download a release You can also use the container image from ghcr.io : ghcr.io/inlets/inlets-pro:latest Your first HTTPS tunnel with an automated tunnel server (Intermediate) \u00b6 Tutorial: Expose one or more local HTTP services via HTTPS Running a HTTP tunnel server manually (Advanced) \u00b6 Tutorial: Setting up a HTTP tunnel server manually Running multiple tunnel servers on the same host (Advanced) \u00b6 The easiest way to scale out inlets tunnels is through the Kubernetes helm chart (see below), however you can manually set up a TCP and HTTPS tunnel on the same machine. Advanced: Setting up dual TCP and HTTPS tunnels Local port forwarding (Intermediate) \u00b6 Case-study: Reliable local port-forwarding from Kubernetes Connecting with Kubernetes \u00b6 You may have an on-premises Kubernetes cluster that needs ingress. Perhaps you have a homelab, or Raspberry Pi cluster, that you want to self host services on. Tutorial: Expose a local IngressController with the inlets-operator Tutorial: Expose Kubernetes services in short-lived clusters with helm Some teams want to have dev work like production, with tools Istio working locally just like in the cloud. * Tutorial: \"A bit of Istio before tea-time\" See also: helm charts Tunnelling TCP services \u00b6 inlets is not limited to HTTP connections, you can also tunnel TCP protocols like RDP, VNC, SSH, TLS and databases. Tutorial: Expose a private SSH server over a TCP tunnel Tutorial: Tunnel a private Postgresql database Reference documentation \u00b6 inletsctl \u00b6 Learn how to use inletsctl to provision tunnel servers on various public clouds. inletsctl reference inlets-operator \u00b6 Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers. inlets-operator reference Other resources \u00b6 For news, use-cases and guides check out the blog: Official Inlets blog Watch a video, or read a blog post from the community: Community tutorials Open Source tools for managing inlets tunnels: Inlets Operator for Kubernetes LoadBalancers inletsctl to provision tunnel servers inlets helm charts for clients and servers Connecting with the inlets community \u00b6 Who built inlets? Inlets \u00ae is a commercial solution developed and supported by OpenFaaS Ltd. You can also contact the team via the contact page . The code for this website is open source and available on GitHub inlets is proud to be featured on the Cloud Native Landscape in the Service Proxy category.","title":"Introduction"},{"location":"#inlets-documentation","text":"Inlets reinvents the concept of a tunnel for a Cloud Native world. You can visit the inlets homepage at https://inlets.dev/ Note Welcome to the new inlets documentation site launched Nov 2021 . This rewrite aims to be less verbose, to connect you with practical task-driven examples and to make it easier to find what you need. If you have comments, questions or suggestions, please let us know on GitHub . With inlets you are in control of your data, unlike with a SaaS tunnel where shared servers mean your data may be at risk. You can use inlets for local development and in your production environment. It works just as well on bare-metal as in VMs, containers and Kubernetes clusters. inlets is not just compatible with tricky networks and Cloud Native architecture, it was purpose-built for them. Common use-cases include: Exposing local endpoints on the Internet Self-hosting from a homelab or on-premises datacenter Deploying and monitoring apps across multiple locations Remote customer support What is your strategy for connecting existing applications to the public cloud? Read: The Simple Way To Connect Existing Apps to Public Cloud .","title":"Inlets documentation"},{"location":"#how-does-it-work","text":"Inlets tunnels connect to each other over a secure websocket with TLS encryption. Over that private connection, you can then tunnel HTTPS or TCP traffic to computers in another network or to the Internet. One of the most common use-cases is to expose a local HTTP endpoint on the Internet via a HTTPS tunnel. You may be working with webhooks, integrating with OAuth, sharing a draft of a blog post or integrating with a partner's API. After deploying an inlets HTTPS server on a public cloud VM, you can then connect the client and access it. There is more that inlets can do for you than exposing local endpoints. inlets also supports local forwarding and can be used to replace more cumbersome services like SSH, complex VPNs or expensive direct connect uplinks. Read more in the: the inlets FAQ .","title":"How does it work?"},{"location":"#getting-started","text":"These guides walk you through a specific use-case with inlets. If you have questions or cannot find what you need, there are options for connecting with the community at the end of this page. Inlets can tunnel either HTTP or TCP traffic: HTTP (L7) tunnels can be used to connect one or more HTTP endpoints from one network to another. A single tunnel can expose multiple websites or hosts, including LoadBalancing and multiple clients to one server. TCP (L4) tunnels can be used to connect TCP services such as a database, a reverse proxy, RDP, Kubernetes or SSH to the Internet. A single tunnel can expose multiple ports on an exit-server and load balance between clients","title":"Getting started"},{"location":"#downloading-inlets","text":"inlets is available for Windows, MacOS (including M1) and Linux (including ARM): Download a release You can also use the container image from ghcr.io : ghcr.io/inlets/inlets-pro:latest","title":"Downloading inlets"},{"location":"#your-first-https-tunnel-with-an-automated-tunnel-server-intermediate","text":"Tutorial: Expose one or more local HTTP services via HTTPS","title":"Your first HTTPS tunnel with an automated tunnel server (Intermediate)"},{"location":"#running-a-http-tunnel-server-manually-advanced","text":"Tutorial: Setting up a HTTP tunnel server manually","title":"Running a HTTP tunnel server manually (Advanced)"},{"location":"#running-multiple-tunnel-servers-on-the-same-host-advanced","text":"The easiest way to scale out inlets tunnels is through the Kubernetes helm chart (see below), however you can manually set up a TCP and HTTPS tunnel on the same machine. Advanced: Setting up dual TCP and HTTPS tunnels","title":"Running multiple tunnel servers on the same host (Advanced)"},{"location":"#local-port-forwarding-intermediate","text":"Case-study: Reliable local port-forwarding from Kubernetes","title":"Local port forwarding (Intermediate)"},{"location":"#connecting-with-kubernetes","text":"You may have an on-premises Kubernetes cluster that needs ingress. Perhaps you have a homelab, or Raspberry Pi cluster, that you want to self host services on. Tutorial: Expose a local IngressController with the inlets-operator Tutorial: Expose Kubernetes services in short-lived clusters with helm Some teams want to have dev work like production, with tools Istio working locally just like in the cloud. * Tutorial: \"A bit of Istio before tea-time\" See also: helm charts","title":"Connecting with Kubernetes"},{"location":"#tunnelling-tcp-services","text":"inlets is not limited to HTTP connections, you can also tunnel TCP protocols like RDP, VNC, SSH, TLS and databases. Tutorial: Expose a private SSH server over a TCP tunnel Tutorial: Tunnel a private Postgresql database","title":"Tunnelling TCP services"},{"location":"#reference-documentation","text":"","title":"Reference documentation"},{"location":"#inletsctl","text":"Learn how to use inletsctl to provision tunnel servers on various public clouds. inletsctl reference","title":"inletsctl"},{"location":"#inlets-operator","text":"Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers. inlets-operator reference","title":"inlets-operator"},{"location":"#other-resources","text":"For news, use-cases and guides check out the blog: Official Inlets blog Watch a video, or read a blog post from the community: Community tutorials Open Source tools for managing inlets tunnels: Inlets Operator for Kubernetes LoadBalancers inletsctl to provision tunnel servers inlets helm charts for clients and servers","title":"Other resources"},{"location":"#connecting-with-the-inlets-community","text":"Who built inlets? Inlets \u00ae is a commercial solution developed and supported by OpenFaaS Ltd. You can also contact the team via the contact page . The code for this website is open source and available on GitHub inlets is proud to be featured on the Cloud Native Landscape in the Service Proxy category.","title":"Connecting with the inlets community"},{"location":"reference/","text":"Reference documentation \u00b6 inletsctl \u00b6 Learn how to use inletsctl to provision tunnel servers on various public clouds. inletsctl reference inlets-operator \u00b6 Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers. inlets-operator reference GitHub repositories \u00b6 inlets-pro inlets-operator inletsctl inlets helm charts","title":"Overview"},{"location":"reference/#reference-documentation","text":"","title":"Reference documentation"},{"location":"reference/#inletsctl","text":"Learn how to use inletsctl to provision tunnel servers on various public clouds. inletsctl reference","title":"inletsctl"},{"location":"reference/#inlets-operator","text":"Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers. inlets-operator reference","title":"inlets-operator"},{"location":"reference/#github-repositories","text":"inlets-pro inlets-operator inletsctl inlets helm charts","title":"GitHub repositories"},{"location":"reference/faq/","text":"Inlets FAQ \u00b6 Inlets concepts and Frequently Asked Questions (FAQ) Why did we build inlets? \u00b6 We built inlets to make it easy to expose a local service on the Internet and to overcome limitations with SaaS tunnels and VPNs. It was built to overcome limitations in SaaS tunnels - such as lack of privacy, control and rate-limits It doesn't just integrate with containers and Kubernetes, it was purpose-built to run in them It's easy to run on Windows, Linux and MacOS with a self-contained binary It doesn't need to run as root, doesn't depend on iptables, doesn't need a tun device or NET_ADMIN capability There are many different networking tools available such as VPNs and SaaS tunnels - each with its own set of pros and cons, and use-cases. It's very likely that you will use several tools together to get the best out of each of them. How does inlets compare to other tools and solutions? \u00b6 Are you curious about the advantages of using inlets vs. alternatives? We must first ask, advantages vs. what other tool or service. SaaS tunnels provide a convenient way to expose services for the purposes of development, however they are often: blocked by corporate IT running on shared infrastructure (servers) with other customers subject to stringent rate-limits that affect productivity priced per subdomain unable to obtain high value TCP ports like 22, 80, 443 and so on You run inlets on your own servers, so you do not run into those restrictions. Your data remains your own and is kept private. When compared to VPNs such as Wireguard, Tailscale and OpenVPN, we have to ask what the use-case is. A traditional VPN is built to connect hosts and entire IP ranges together . This can potentially expose a large number of machines and users to each other and requires complex Access Control Lists or authorization rules. If this is your use-case, a traditional VPN is probably the right tool for the job. Inlets is designed to connect or expose services between networks - either HTTP or TCP. For example: Receiving webhooks to a local application Sharing a blog post draft with a colleague or client Providing remote access to your homelab when away from home Self-hosting websites or services on Kubernetes clusters Getting working LoadBalancers with public IPs for local Kubernetes clusters You can also use inlets to replace Direct Connect or a VPN when you just need to connect a number of services privately and not an entire network range. Many of the inlets community use a VPN alongside inlets, because they are different tools for different use-cases. We often write about use-cases for public and private inlets tunnels on the blog . What's the difference between inlets, inletsctl and inlets-operator? \u00b6 inlets-pro aka \"inlets\" is the command-line tool that contains both the client and server required to set up HTTP and TCP tunnels. The inlets-pro server is usually set up on a computer with a public IP address, then the inlets-pro client is run on your own machine, or a separate computer that can reach the service or server you want to expose. You can download inlets-pro and inletsctl with the \"curl | sh\" commands provided at the start of each tutorial, this works best on a Linux host, or with Git Bash if using Windows. Did you know? You can also download binaries for inlets-pro and inletsctl on GitHub, for Windows users you'll want \"inlets-pro.exe\" and for MacOS, you'll want \"inlets-pro-darwin\". For instance, on Windows machines you'll need \"inlets-pro.exe\" See also: inlets-pro releases inletsctl is a tool that can set up a tunnel server for you on around a dozen popular clouds. It exists to make it quicker and more convenience to set up a HTTPS or TCP tunnel to expose a local service. It has three jobs: 1) Create the VM for you 2) Install the inlets-pro server in TCP or HTTPS mode (as specified) with systemd 3) Inform you of the token and connection string You can download the inletsctl tool with \"curl | sh\" or from the inletsctl releases page. Find out more: inletsctl reference page inlets-operator is a Kubernetes Operator that will create tunnel servers for you, on your chosen cloud for any LoadBalancers that you expose within a private cluster. Find out more: inlets-operator reference page What is the networking model for inlets? \u00b6 Whilst some networking tools such as Bittorrent use a peer-to-peer network, inlets uses a more traditional client/server model. One or more client tunnels connect to a tunnel server and advertise which services they are able to provide. Then, whenever the server receives traffic for one of those advertised services, it will forward it through the tunnel to the client. The tunnel client will then forward that on to the service it advertised. The tunnel server may also be referred to as an \"exit\" server because it is the connection point for the client to another network or the Internet. If you install and run the inlets server on a computer, it can be referred to as a tunnel server or exit server . These servers can also be automated through cloud-init, terraform, or tools maintained by the inlets community such as inletsctl . Pictured: the website http://127.0.0.1:3000 is exposed through an encrypted tunnel to users at: https://example.com For remote forwarding, the client tends to be run within a private network, with an --upstream flag used to specify where incoming traffic needs to be routed. The tunnel server can then be run on an Internet-facing network, or any other network reachable by the client. What kind of layers and protocols are supported? \u00b6 Inlets works at a higher level than traditional VPNs because it is designed to connect services together, rather than hosts directly. HTTP - Layer 7 of the OSI model, used for web traffic such as websites and RESTful APIs TCP - Layer 4 of the OSI model, used for TCP traffic like SSH, TLS, databases, RDP, etc Because VPNs are designed to connect hosts together over a shared IP space, they also involve tedious IP address management and allocation. Inlets connects services, so for TCP traffic, you need only think about TCP ports. For HTTP traffic, you need only to think about domain names. Do I want a TCP or HTTPS tunnel? \u00b6 If you're exposing websites, blogs, docs, APIs and webhooks, you should use a HTTPS tunnel. For HTTP tunnels, Rate Error and Duration (RED) metrics are collected for any service you expose, even if it doesn't have its own instrumentation support. For anything that doesn't fit into that model, a TCP tunnel may be a better option. Common examples are: RDP, VNC, SSH, TLS, database protocols, legacy medical protocols such as DiCom. TCP tunnels can also be used to forward traffic to a reverse proxy like Nginx, Caddy, or Traefik, sitting behind a firewall or NAT by forwarding port 80 and 443. TCP traffic is forwarded directly between the two hosts without any decryption of bytes. The active connection count and frequency can be monitored along with the amount of throughput. Does inlets use TCP or UDP? \u00b6 Inlets uses a websocket over TCP, so that it can penetrate HTTP proxies, captive portals, firewalls, and other kinds of NAT. As long as the client can make an outbound connection, a tunnel can be established. The use of HTTPS means that inlets will have similar latency and throughput to a HTTPS server or SSH tunnel. Once you have an inlets tunnel established, you can use it to tunnel traffic to TCP and HTTPS sockets within the private network of the client. Most VPNs tend to use UDP for communication due to its low overhead which results in lower latency and higher throughput. Certain tools and products such as OpenVPN, SSH and Tailscale can be configured to emulate a TCP stack over a TCP connection, this can lead to unexpected issues . Inlets connections send data, rather than emulating a TCP over TCP stack, so doesn't suffer from this problem. Are both remote and local forwarding supported? \u00b6 Remote forwarding is where a local service is forwarded from the client's network to the inlets tunnel server. Remote forwarding pushes a local endpoint to a remote host for access on another network This is the most common use-case and would be used to expose a local HTTP server to the public Internet via a tunnel server. Local forwarding is used to forward a service on the tunnel server or tunnel server's network back to the client, so that it can be accessed using a port on localhost. Local forwarding brings a remote service back to localhost for accessing An example would be that you have a webserver and MySQL database. The HTTP server is public and can access the database via its own loopback adapter, but the Internet cannot. So how do you access that MySQL database from CI, or from your local machine? Connect a client with local forwarding, and bring the MySQL port back to your local machine or CI runner, and then use the MySQL CLI to access it. A developer at the UK Government uses inlets to forward a NATS message queue from a staging environment to his local machine for testing. Learn more What's the difference between the data plane and control plane? \u00b6 The data plane is any service or port that carries traffic from the tunnel server to the tunnel client, and your private TCP or HTTP services. It can be exposed on all interfaces, or only bound to loopback for private access, in a similar way to a VPN. If you were exposing SSH on an internal machine from port 2222 , your data-plane may be exposed on port 2222 The control-plane is a TLS-encrypted, authenticated websocket that is used to connect clients to servers. All traffic ultimately passes over the control-plane's link, so remains encrypted and private. Your control-plane's port is usually 8123 when used directly, or 443 when used behind a reverse proxy or Kubernetes Ingress Controller. An example from the article: The Simple Way To Connect Existing Apps to Public Cloud A legacy MSSQL server runs on Windows Server behind the firewall in a private datacenter. Your organisation cannot risk migrating it to an AWS EC2 instance at this time, but can move the microservice that needs to access it. The inlets tunnel allows for the MSSQL service to be tunneled privately to the EC2 instance's local network for accessing, but is not exposed on the Internet. All traffic is encrypted over the wire due to the TLS connection of inlets. Hybrid Cloud in action using an inlets tunnel to access the on-premises database This concept is referred to as a a \"split plane\" because the control plane is available to public clients on all adapters, and the data plane is only available on local or private adapters on the server. Is there a reference guide to the CLI? \u00b6 The inlets-pro binary has built-in help commands and examples, just run inlets-pro tcp/http client/server --help . A separate CLI reference guide is also available here: inlets-pro CLI reference Is inlets secure? \u00b6 All traffic sent over an inlets tunnel is encapsulated in a TLS-encrypted websocket, which prevents eavesdropping. The tunnel client is authenticated using an API token which is generated by the tunnel administrator, or by automated tooling. Additional authentication mechanisms can be set up using a reverse proxy such as Nginx. Do I have to expose services on the Internet to use inlets? \u00b6 No, inlets can be used to tunnel one or more services to another network without exposing them on the Internet. The --data-addr 127.0.0.1: flag for inlets servers binds the data plane to the server's loopback address, meaning that only other processing running on it can access the tunneled services. You could also use a private network adapter or VPC IP address in the --data-addr flag. How do I monitor inlets? \u00b6 See the following blog post for details on the inlets status command and the various Prometheus metrics that are made available. Measure and monitor your inlets tunnels How do you scale inlets? \u00b6 Inlets HTTP servers can support a high number of clients, either for load-balancing the same internal service to a number of clients, or for a number of distinct endpoints. Tunnel servers are easy to scale through the use of containers, and can benefit from the resilience that a Kubernetes cluster can bring: See also: How we scaled inlets to thousands of tunnels with Kubernetes Does inlets support High Availability (HA)? \u00b6 For the inlets client, it is possible to connect multiple inlets tunnel clients for the same service, such as a company blog. Traffic will be distributed across the clients and if one of those clients goes down or crashes, the other will continue to serve requests. For the inlets tunnel server, the easiest option is to run the server in a supervisor that can restart the tunnel service quickly or allow it to run more than one replica. Systemd can be used to restart tunnel servers should they run into issues, likewise you can run the server in a container, or as a Kubernetes Pod. HA example with an AWS ELB For example, you may place a cloud load-balancer in front of the data-plane port of two inlets server processes. Requests to the stable load-balancer IP address will be distributed between the two virtual machines and their respective inlets server tunnel processes. Is IPv6 supported? \u00b6 Yes, see also: How to serve traffic to IPv6 users with inlets What if the websocket disconnects? \u00b6 The client will reconnect automatically and can be configured with systemd or a Windows service to stay running in the background. See also inlets pro tcp/http server/client --generate=systemd for generating systemd unit files. When used in combination with a Kubernetes ingress controller or reverse proxy of your own, then the websocket may timeout. These timeout settings can usually be configured to remove any potential issue. Monitoring in inlets allows for you to monitor the reliability of your clients and servers, which are often running in distinct networks. How much does inlets cost? \u00b6 Monthly and annual subscriptions are available via Gumroad. You can also purchase a static license for offline or air-gapped environments. For more, see the Pricing page What happens when the license expires? \u00b6 If you're using a Gumroad license, and keep your billing relationship active, then the software will work for as long as you keep paying. The Gumroad license server needs to be reachable by the inlets client. If you're using a static license, then the software will continue to run, even after your license has expired, unless you restart the software. You can either rotate the token on your inlets clients in an automated or manual fashion, or purchase a token for a longer period of time up front. Can I get professional help? \u00b6 Inlets is designed to be self-service and is well documented, but perhaps you could use some direction? Business licenses come with support via email, however you are welcome to contact OpenFaaS Ltd to ask about a consulting project.","title":"Inlets FAQ"},{"location":"reference/faq/#inlets-faq","text":"Inlets concepts and Frequently Asked Questions (FAQ)","title":"Inlets FAQ"},{"location":"reference/faq/#why-did-we-build-inlets","text":"We built inlets to make it easy to expose a local service on the Internet and to overcome limitations with SaaS tunnels and VPNs. It was built to overcome limitations in SaaS tunnels - such as lack of privacy, control and rate-limits It doesn't just integrate with containers and Kubernetes, it was purpose-built to run in them It's easy to run on Windows, Linux and MacOS with a self-contained binary It doesn't need to run as root, doesn't depend on iptables, doesn't need a tun device or NET_ADMIN capability There are many different networking tools available such as VPNs and SaaS tunnels - each with its own set of pros and cons, and use-cases. It's very likely that you will use several tools together to get the best out of each of them.","title":"Why did we build inlets?"},{"location":"reference/faq/#how-does-inlets-compare-to-other-tools-and-solutions","text":"Are you curious about the advantages of using inlets vs. alternatives? We must first ask, advantages vs. what other tool or service. SaaS tunnels provide a convenient way to expose services for the purposes of development, however they are often: blocked by corporate IT running on shared infrastructure (servers) with other customers subject to stringent rate-limits that affect productivity priced per subdomain unable to obtain high value TCP ports like 22, 80, 443 and so on You run inlets on your own servers, so you do not run into those restrictions. Your data remains your own and is kept private. When compared to VPNs such as Wireguard, Tailscale and OpenVPN, we have to ask what the use-case is. A traditional VPN is built to connect hosts and entire IP ranges together . This can potentially expose a large number of machines and users to each other and requires complex Access Control Lists or authorization rules. If this is your use-case, a traditional VPN is probably the right tool for the job. Inlets is designed to connect or expose services between networks - either HTTP or TCP. For example: Receiving webhooks to a local application Sharing a blog post draft with a colleague or client Providing remote access to your homelab when away from home Self-hosting websites or services on Kubernetes clusters Getting working LoadBalancers with public IPs for local Kubernetes clusters You can also use inlets to replace Direct Connect or a VPN when you just need to connect a number of services privately and not an entire network range. Many of the inlets community use a VPN alongside inlets, because they are different tools for different use-cases. We often write about use-cases for public and private inlets tunnels on the blog .","title":"How does inlets compare to other tools and solutions?"},{"location":"reference/faq/#whats-the-difference-between-inlets-inletsctl-and-inlets-operator","text":"inlets-pro aka \"inlets\" is the command-line tool that contains both the client and server required to set up HTTP and TCP tunnels. The inlets-pro server is usually set up on a computer with a public IP address, then the inlets-pro client is run on your own machine, or a separate computer that can reach the service or server you want to expose. You can download inlets-pro and inletsctl with the \"curl | sh\" commands provided at the start of each tutorial, this works best on a Linux host, or with Git Bash if using Windows. Did you know? You can also download binaries for inlets-pro and inletsctl on GitHub, for Windows users you'll want \"inlets-pro.exe\" and for MacOS, you'll want \"inlets-pro-darwin\". For instance, on Windows machines you'll need \"inlets-pro.exe\" See also: inlets-pro releases inletsctl is a tool that can set up a tunnel server for you on around a dozen popular clouds. It exists to make it quicker and more convenience to set up a HTTPS or TCP tunnel to expose a local service. It has three jobs: 1) Create the VM for you 2) Install the inlets-pro server in TCP or HTTPS mode (as specified) with systemd 3) Inform you of the token and connection string You can download the inletsctl tool with \"curl | sh\" or from the inletsctl releases page. Find out more: inletsctl reference page inlets-operator is a Kubernetes Operator that will create tunnel servers for you, on your chosen cloud for any LoadBalancers that you expose within a private cluster. Find out more: inlets-operator reference page","title":"What's the difference between inlets, inletsctl and inlets-operator?"},{"location":"reference/faq/#what-is-the-networking-model-for-inlets","text":"Whilst some networking tools such as Bittorrent use a peer-to-peer network, inlets uses a more traditional client/server model. One or more client tunnels connect to a tunnel server and advertise which services they are able to provide. Then, whenever the server receives traffic for one of those advertised services, it will forward it through the tunnel to the client. The tunnel client will then forward that on to the service it advertised. The tunnel server may also be referred to as an \"exit\" server because it is the connection point for the client to another network or the Internet. If you install and run the inlets server on a computer, it can be referred to as a tunnel server or exit server . These servers can also be automated through cloud-init, terraform, or tools maintained by the inlets community such as inletsctl . Pictured: the website http://127.0.0.1:3000 is exposed through an encrypted tunnel to users at: https://example.com For remote forwarding, the client tends to be run within a private network, with an --upstream flag used to specify where incoming traffic needs to be routed. The tunnel server can then be run on an Internet-facing network, or any other network reachable by the client.","title":"What is the networking model for inlets?"},{"location":"reference/faq/#what-kind-of-layers-and-protocols-are-supported","text":"Inlets works at a higher level than traditional VPNs because it is designed to connect services together, rather than hosts directly. HTTP - Layer 7 of the OSI model, used for web traffic such as websites and RESTful APIs TCP - Layer 4 of the OSI model, used for TCP traffic like SSH, TLS, databases, RDP, etc Because VPNs are designed to connect hosts together over a shared IP space, they also involve tedious IP address management and allocation. Inlets connects services, so for TCP traffic, you need only think about TCP ports. For HTTP traffic, you need only to think about domain names.","title":"What kind of layers and protocols are supported?"},{"location":"reference/faq/#do-i-want-a-tcp-or-https-tunnel","text":"If you're exposing websites, blogs, docs, APIs and webhooks, you should use a HTTPS tunnel. For HTTP tunnels, Rate Error and Duration (RED) metrics are collected for any service you expose, even if it doesn't have its own instrumentation support. For anything that doesn't fit into that model, a TCP tunnel may be a better option. Common examples are: RDP, VNC, SSH, TLS, database protocols, legacy medical protocols such as DiCom. TCP tunnels can also be used to forward traffic to a reverse proxy like Nginx, Caddy, or Traefik, sitting behind a firewall or NAT by forwarding port 80 and 443. TCP traffic is forwarded directly between the two hosts without any decryption of bytes. The active connection count and frequency can be monitored along with the amount of throughput.","title":"Do I want a TCP or HTTPS tunnel?"},{"location":"reference/faq/#does-inlets-use-tcp-or-udp","text":"Inlets uses a websocket over TCP, so that it can penetrate HTTP proxies, captive portals, firewalls, and other kinds of NAT. As long as the client can make an outbound connection, a tunnel can be established. The use of HTTPS means that inlets will have similar latency and throughput to a HTTPS server or SSH tunnel. Once you have an inlets tunnel established, you can use it to tunnel traffic to TCP and HTTPS sockets within the private network of the client. Most VPNs tend to use UDP for communication due to its low overhead which results in lower latency and higher throughput. Certain tools and products such as OpenVPN, SSH and Tailscale can be configured to emulate a TCP stack over a TCP connection, this can lead to unexpected issues . Inlets connections send data, rather than emulating a TCP over TCP stack, so doesn't suffer from this problem.","title":"Does inlets use TCP or UDP?"},{"location":"reference/faq/#are-both-remote-and-local-forwarding-supported","text":"Remote forwarding is where a local service is forwarded from the client's network to the inlets tunnel server. Remote forwarding pushes a local endpoint to a remote host for access on another network This is the most common use-case and would be used to expose a local HTTP server to the public Internet via a tunnel server. Local forwarding is used to forward a service on the tunnel server or tunnel server's network back to the client, so that it can be accessed using a port on localhost. Local forwarding brings a remote service back to localhost for accessing An example would be that you have a webserver and MySQL database. The HTTP server is public and can access the database via its own loopback adapter, but the Internet cannot. So how do you access that MySQL database from CI, or from your local machine? Connect a client with local forwarding, and bring the MySQL port back to your local machine or CI runner, and then use the MySQL CLI to access it. A developer at the UK Government uses inlets to forward a NATS message queue from a staging environment to his local machine for testing. Learn more","title":"Are both remote and local forwarding supported?"},{"location":"reference/faq/#whats-the-difference-between-the-data-plane-and-control-plane","text":"The data plane is any service or port that carries traffic from the tunnel server to the tunnel client, and your private TCP or HTTP services. It can be exposed on all interfaces, or only bound to loopback for private access, in a similar way to a VPN. If you were exposing SSH on an internal machine from port 2222 , your data-plane may be exposed on port 2222 The control-plane is a TLS-encrypted, authenticated websocket that is used to connect clients to servers. All traffic ultimately passes over the control-plane's link, so remains encrypted and private. Your control-plane's port is usually 8123 when used directly, or 443 when used behind a reverse proxy or Kubernetes Ingress Controller. An example from the article: The Simple Way To Connect Existing Apps to Public Cloud A legacy MSSQL server runs on Windows Server behind the firewall in a private datacenter. Your organisation cannot risk migrating it to an AWS EC2 instance at this time, but can move the microservice that needs to access it. The inlets tunnel allows for the MSSQL service to be tunneled privately to the EC2 instance's local network for accessing, but is not exposed on the Internet. All traffic is encrypted over the wire due to the TLS connection of inlets. Hybrid Cloud in action using an inlets tunnel to access the on-premises database This concept is referred to as a a \"split plane\" because the control plane is available to public clients on all adapters, and the data plane is only available on local or private adapters on the server.","title":"What's the difference between the data plane and control plane?"},{"location":"reference/faq/#is-there-a-reference-guide-to-the-cli","text":"The inlets-pro binary has built-in help commands and examples, just run inlets-pro tcp/http client/server --help . A separate CLI reference guide is also available here: inlets-pro CLI reference","title":"Is there a reference guide to the CLI?"},{"location":"reference/faq/#is-inlets-secure","text":"All traffic sent over an inlets tunnel is encapsulated in a TLS-encrypted websocket, which prevents eavesdropping. The tunnel client is authenticated using an API token which is generated by the tunnel administrator, or by automated tooling. Additional authentication mechanisms can be set up using a reverse proxy such as Nginx.","title":"Is inlets secure?"},{"location":"reference/faq/#do-i-have-to-expose-services-on-the-internet-to-use-inlets","text":"No, inlets can be used to tunnel one or more services to another network without exposing them on the Internet. The --data-addr 127.0.0.1: flag for inlets servers binds the data plane to the server's loopback address, meaning that only other processing running on it can access the tunneled services. You could also use a private network adapter or VPC IP address in the --data-addr flag.","title":"Do I have to expose services on the Internet to use inlets?"},{"location":"reference/faq/#how-do-i-monitor-inlets","text":"See the following blog post for details on the inlets status command and the various Prometheus metrics that are made available. Measure and monitor your inlets tunnels","title":"How do I monitor inlets?"},{"location":"reference/faq/#how-do-you-scale-inlets","text":"Inlets HTTP servers can support a high number of clients, either for load-balancing the same internal service to a number of clients, or for a number of distinct endpoints. Tunnel servers are easy to scale through the use of containers, and can benefit from the resilience that a Kubernetes cluster can bring: See also: How we scaled inlets to thousands of tunnels with Kubernetes","title":"How do you scale inlets?"},{"location":"reference/faq/#does-inlets-support-high-availability-ha","text":"For the inlets client, it is possible to connect multiple inlets tunnel clients for the same service, such as a company blog. Traffic will be distributed across the clients and if one of those clients goes down or crashes, the other will continue to serve requests. For the inlets tunnel server, the easiest option is to run the server in a supervisor that can restart the tunnel service quickly or allow it to run more than one replica. Systemd can be used to restart tunnel servers should they run into issues, likewise you can run the server in a container, or as a Kubernetes Pod. HA example with an AWS ELB For example, you may place a cloud load-balancer in front of the data-plane port of two inlets server processes. Requests to the stable load-balancer IP address will be distributed between the two virtual machines and their respective inlets server tunnel processes.","title":"Does inlets support High Availability (HA)?"},{"location":"reference/faq/#is-ipv6-supported","text":"Yes, see also: How to serve traffic to IPv6 users with inlets","title":"Is IPv6 supported?"},{"location":"reference/faq/#what-if-the-websocket-disconnects","text":"The client will reconnect automatically and can be configured with systemd or a Windows service to stay running in the background. See also inlets pro tcp/http server/client --generate=systemd for generating systemd unit files. When used in combination with a Kubernetes ingress controller or reverse proxy of your own, then the websocket may timeout. These timeout settings can usually be configured to remove any potential issue. Monitoring in inlets allows for you to monitor the reliability of your clients and servers, which are often running in distinct networks.","title":"What if the websocket disconnects?"},{"location":"reference/faq/#how-much-does-inlets-cost","text":"Monthly and annual subscriptions are available via Gumroad. You can also purchase a static license for offline or air-gapped environments. For more, see the Pricing page","title":"How much does inlets cost?"},{"location":"reference/faq/#what-happens-when-the-license-expires","text":"If you're using a Gumroad license, and keep your billing relationship active, then the software will work for as long as you keep paying. The Gumroad license server needs to be reachable by the inlets client. If you're using a static license, then the software will continue to run, even after your license has expired, unless you restart the software. You can either rotate the token on your inlets clients in an automated or manual fashion, or purchase a token for a longer period of time up front.","title":"What happens when the license expires?"},{"location":"reference/faq/#can-i-get-professional-help","text":"Inlets is designed to be self-service and is well documented, but perhaps you could use some direction? Business licenses come with support via email, however you are welcome to contact OpenFaaS Ltd to ask about a consulting project.","title":"Can I get professional help?"},{"location":"reference/inlets-operator/","text":"inlets-operator reference documentation \u00b6 The inlets/inlets-operator brings LoadBalancers with public IP addresses to your local Kubernetes clusters. It works by creating VMs and running an inlets PRO tunnel server for you, the VM's public IP is then attached to the cluster and an inlets client Pod runs for you. You can install the inlets-operator using a single command with arkade or with helm. arkade is an open-source Kubernetes marketplace and easier to use. For each provider, the minimum requirements tend to be: An access token - for the operator to create VMs for inlets PRO servers A region - where to create the VMs You can subscribe to inlets for personal or commercial use via Gumroad Install using arkade \u00b6 arkade install inlets-operator \\ --provider $PROVIDER \\ # Name of the cloud provider to provision the exit-node on. --region $REGION \\ # Used with cloud providers that require a region. --zone $ZONE \\ # Used with cloud providers that require zone (e.g. gce). --token-file $HOME /Downloads/key.json \\ # Token file/Service Account Key file with the access to the cloud provider. --license-file inlets-pro-license.txt # inlets-pro license file. (required) Install using helm \u00b6 Checkout the inlets-operator helm chart README to know more about the values that can be passed to --set and to see provider specific example commands. # Create the Custom Resource Definition for a \"Tunnel\" kubectl apply -f \\ https://raw.githubusercontent.com/inlets/inlets-operator/master/artifacts/crds/inlets.inlets.dev_tunnels.yaml # Create a secret to store the service account key file kubectl create secret generic inlets-access-key \\ --from-file = inlets-access-key = key.json # Add and update the inlets-operator helm repo helm repo add inlets https://inlets.github.io/inlets-operator/ # Update the local repository helm repo update # Install inlets-operator with the required fields helm upgrade inlets-operator --install inlets/inlets-operator \\ --set provider = $PROJECTID ,zone = $ZONE ,region = $REGION \\ --set projectID = $PROJECTID \\ --set inletsProLicense = $LICENSE View the code and chart on GitHub: inlets/inlets-operator Instructions per cloud \u00b6 Create tunnel servers on DigitalOcean \u00b6 Install with inlets PRO on DigitalOcean . Assuming you have created an API key and saved it to $HOME/Downloads/do-access-token , run: arkade install inlets-operator \\ --provider digitalocean \\ --region lon1 \\ --token-file $HOME /Downloads/do-access-token \\ --license-file $HOME /.inlets/LICENSE Create tunnel servers on AWS EC2 \u00b6 Instructions for AWS EC2 To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles. Create a AWS IAM Policy with the following: Create a file named policy.json with the following content { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:DescribeInstances\" , \"ec2:DescribeImages\" , \"ec2:TerminateInstances\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateTags\" , \"ec2:DeleteSecurityGroup\" , \"ec2:RunInstances\" , \"ec2:DescribeInstanceStatus\" ], \"Resource\" : [ \"*\" ] } ] } Create the policy in AWS aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json Create an IAM user aws iam create-user --user-name inlets-automation Add the Policy to the IAM user We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below. export AWS_ACCOUNT_NUMBER = \"Your AWS Account Number\" aws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam:: ${ AWS_ACCOUNT_NUMBER } :policy/inlets-automation Generate an access key for your IAM User The below commands will create a set of credentials and save them into files for use later on. we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually. ACCESS_KEY_JSON = $( aws iam create-access-key --user-name inlets-automation ) echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId > access-key echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey > secret-access-key Install with inlets PRO: arkade install inlets-operator \\ --provider ec2 \\ --region eu-west-1 \\ --token-file $HOME /Downloads/access-key \\ --secret-key-file $HOME /Downloads/secret-access-key \\ --license-file $HOME /.inlets/LICENSE Create tunnel servers on Google Compute Engine (GCE) \u00b6 Instructions for Google Cloud It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here To get your service account key file, follow the steps below: # Get current projectID export PROJECTID = $( gcloud config get-value core/project 2 >/dev/null ) # Create a service account gcloud iam service-accounts create inlets \\ --description \"inlets-operator service account\" \\ --display-name \"inlets\" # Get service account email export SERVICEACCOUNT = $( gcloud iam service-accounts list | grep inlets | awk '{print $2}' ) # Assign appropriate roles to inlets service account gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/compute.admin gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/iam.serviceAccountUser # Create inlets service account key file gcloud iam service-accounts keys create key.json \\ --iam-account $SERVICEACCOUNT Install the operator: arkade install inlets-operator \\ --provider gce \\ --project-id $PROJECTID \\ --zone us-central1-a \\ --token-file key.json \\ --license-file $HOME /.inlets/LICENSE Create tunnel servers on Azure \u00b6 Instructions for Azure Prerequisites: You will need az . See Install the Azure CLI You'll need to have run az login also Generate Azure authentication file: az ad sp create-for-rbac --sdk-auth \\ > $HOME /Downloads/client_credentials.json Find your region code with: az account list-locations -o table DisplayName Name RegionalDisplayName ------------------------ ------------------- ------------------------------------- United Kingdom ukwest United Kingdom Install using helm: export SUBSCRIPTION_ID = \"YOUR_SUBSCRIPTION_ID\" export AZURE_REGION = \"ukwest\" export INLETS_LICENSE = \" $( cat ~/.inlets/LICENSE ) \" export ACCESS_KEY = \" $HOME /Downloads/client_credentials.json\" kubectl apply -f \\ https://raw.githubusercontent.com/inlets/inlets-operator/master/artifacts/crds/inlets.inlets.dev_tunnels.yaml kubectl create secret generic inlets-access-key \\ --from-file = inlets-access-key = $ACCESS_KEY helm repo add inlets https://inlets.github.io/inlets-operator/ helm repo update helm upgrade inlets-operator --install inlets/inlets-operator \\ --set provider = azure,region = $AZURE_REGION \\ --set subscriptionID = $SUBSCRIPTION_ID \\ --set inletsProLicense = $INLETS_LICENSE Create tunnel servers on Linode \u00b6 Instructions for Linode Install using helm: kubectl apply -f ./artifacts/crds/ # Create a secret to store the service account key file kubectl create secret generic inlets-access-key --from-literal inlets-access-key = <Linode API Access Key> # Add and update the inlets-operator helm repo helm repo add inlets https://inlets.github.io/inlets-operator/ helm repo update # Install inlets-operator with the required fields helm upgrade inlets-operator --install inlets/inlets-operator \\ --set provider = linode,region = us-east,inletsProLicense = $( cat $HOME /inlets-pro-license.txt ) You can also install the inlets-operator using a single command using arkade , arkade runs against any Kubernetes cluster. Install with inlets PRO: arkade install inlets-operator \\ --provider linode \\ --region us-east \\ --access-key <Linode API Access Key> \\ --license $( cat $HOME /inlets-pro-license.txt )","title":"inlets-operator reference documentation"},{"location":"reference/inlets-operator/#inlets-operator-reference-documentation","text":"The inlets/inlets-operator brings LoadBalancers with public IP addresses to your local Kubernetes clusters. It works by creating VMs and running an inlets PRO tunnel server for you, the VM's public IP is then attached to the cluster and an inlets client Pod runs for you. You can install the inlets-operator using a single command with arkade or with helm. arkade is an open-source Kubernetes marketplace and easier to use. For each provider, the minimum requirements tend to be: An access token - for the operator to create VMs for inlets PRO servers A region - where to create the VMs You can subscribe to inlets for personal or commercial use via Gumroad","title":"inlets-operator reference documentation"},{"location":"reference/inlets-operator/#install-using-arkade","text":"arkade install inlets-operator \\ --provider $PROVIDER \\ # Name of the cloud provider to provision the exit-node on. --region $REGION \\ # Used with cloud providers that require a region. --zone $ZONE \\ # Used with cloud providers that require zone (e.g. gce). --token-file $HOME /Downloads/key.json \\ # Token file/Service Account Key file with the access to the cloud provider. --license-file inlets-pro-license.txt # inlets-pro license file. (required)","title":"Install using arkade"},{"location":"reference/inlets-operator/#install-using-helm","text":"Checkout the inlets-operator helm chart README to know more about the values that can be passed to --set and to see provider specific example commands. # Create the Custom Resource Definition for a \"Tunnel\" kubectl apply -f \\ https://raw.githubusercontent.com/inlets/inlets-operator/master/artifacts/crds/inlets.inlets.dev_tunnels.yaml # Create a secret to store the service account key file kubectl create secret generic inlets-access-key \\ --from-file = inlets-access-key = key.json # Add and update the inlets-operator helm repo helm repo add inlets https://inlets.github.io/inlets-operator/ # Update the local repository helm repo update # Install inlets-operator with the required fields helm upgrade inlets-operator --install inlets/inlets-operator \\ --set provider = $PROJECTID ,zone = $ZONE ,region = $REGION \\ --set projectID = $PROJECTID \\ --set inletsProLicense = $LICENSE View the code and chart on GitHub: inlets/inlets-operator","title":"Install using helm"},{"location":"reference/inlets-operator/#instructions-per-cloud","text":"","title":"Instructions per cloud"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-digitalocean","text":"Install with inlets PRO on DigitalOcean . Assuming you have created an API key and saved it to $HOME/Downloads/do-access-token , run: arkade install inlets-operator \\ --provider digitalocean \\ --region lon1 \\ --token-file $HOME /Downloads/do-access-token \\ --license-file $HOME /.inlets/LICENSE","title":"Create tunnel servers on DigitalOcean"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-aws-ec2","text":"Instructions for AWS EC2 To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles. Create a AWS IAM Policy with the following: Create a file named policy.json with the following content { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:DescribeInstances\" , \"ec2:DescribeImages\" , \"ec2:TerminateInstances\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateTags\" , \"ec2:DeleteSecurityGroup\" , \"ec2:RunInstances\" , \"ec2:DescribeInstanceStatus\" ], \"Resource\" : [ \"*\" ] } ] } Create the policy in AWS aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json Create an IAM user aws iam create-user --user-name inlets-automation Add the Policy to the IAM user We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below. export AWS_ACCOUNT_NUMBER = \"Your AWS Account Number\" aws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam:: ${ AWS_ACCOUNT_NUMBER } :policy/inlets-automation Generate an access key for your IAM User The below commands will create a set of credentials and save them into files for use later on. we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually. ACCESS_KEY_JSON = $( aws iam create-access-key --user-name inlets-automation ) echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId > access-key echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey > secret-access-key Install with inlets PRO: arkade install inlets-operator \\ --provider ec2 \\ --region eu-west-1 \\ --token-file $HOME /Downloads/access-key \\ --secret-key-file $HOME /Downloads/secret-access-key \\ --license-file $HOME /.inlets/LICENSE","title":"Create tunnel servers on AWS EC2"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-google-compute-engine-gce","text":"Instructions for Google Cloud It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here To get your service account key file, follow the steps below: # Get current projectID export PROJECTID = $( gcloud config get-value core/project 2 >/dev/null ) # Create a service account gcloud iam service-accounts create inlets \\ --description \"inlets-operator service account\" \\ --display-name \"inlets\" # Get service account email export SERVICEACCOUNT = $( gcloud iam service-accounts list | grep inlets | awk '{print $2}' ) # Assign appropriate roles to inlets service account gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/compute.admin gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/iam.serviceAccountUser # Create inlets service account key file gcloud iam service-accounts keys create key.json \\ --iam-account $SERVICEACCOUNT Install the operator: arkade install inlets-operator \\ --provider gce \\ --project-id $PROJECTID \\ --zone us-central1-a \\ --token-file key.json \\ --license-file $HOME /.inlets/LICENSE","title":"Create tunnel servers on Google Compute Engine (GCE)"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-azure","text":"Instructions for Azure Prerequisites: You will need az . See Install the Azure CLI You'll need to have run az login also Generate Azure authentication file: az ad sp create-for-rbac --sdk-auth \\ > $HOME /Downloads/client_credentials.json Find your region code with: az account list-locations -o table DisplayName Name RegionalDisplayName ------------------------ ------------------- ------------------------------------- United Kingdom ukwest United Kingdom Install using helm: export SUBSCRIPTION_ID = \"YOUR_SUBSCRIPTION_ID\" export AZURE_REGION = \"ukwest\" export INLETS_LICENSE = \" $( cat ~/.inlets/LICENSE ) \" export ACCESS_KEY = \" $HOME /Downloads/client_credentials.json\" kubectl apply -f \\ https://raw.githubusercontent.com/inlets/inlets-operator/master/artifacts/crds/inlets.inlets.dev_tunnels.yaml kubectl create secret generic inlets-access-key \\ --from-file = inlets-access-key = $ACCESS_KEY helm repo add inlets https://inlets.github.io/inlets-operator/ helm repo update helm upgrade inlets-operator --install inlets/inlets-operator \\ --set provider = azure,region = $AZURE_REGION \\ --set subscriptionID = $SUBSCRIPTION_ID \\ --set inletsProLicense = $INLETS_LICENSE","title":"Create tunnel servers on Azure"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-linode","text":"Instructions for Linode Install using helm: kubectl apply -f ./artifacts/crds/ # Create a secret to store the service account key file kubectl create secret generic inlets-access-key --from-literal inlets-access-key = <Linode API Access Key> # Add and update the inlets-operator helm repo helm repo add inlets https://inlets.github.io/inlets-operator/ helm repo update # Install inlets-operator with the required fields helm upgrade inlets-operator --install inlets/inlets-operator \\ --set provider = linode,region = us-east,inletsProLicense = $( cat $HOME /inlets-pro-license.txt ) You can also install the inlets-operator using a single command using arkade , arkade runs against any Kubernetes cluster. Install with inlets PRO: arkade install inlets-operator \\ --provider linode \\ --region us-east \\ --access-key <Linode API Access Key> \\ --license $( cat $HOME /inlets-pro-license.txt )","title":"Create tunnel servers on Linode"},{"location":"reference/inletsctl/","text":"inletsctl reference documentation \u00b6 inletsctl is an automation tool for inlets/-pro. Features: * create / delete cloud VMs with inlets/-pro pre-installed via systemd * download [--pro] - download the inlets/-pro binaries to your local computer * kfwd - forward services from a Kubernetes cluster to your local machine using inlets/-pro View the code on GitHub: inlets/inletsctl Install inletsctl \u00b6 You can install inletsctl using its installer, or from the GitHub releases page # Install to local directory (and for Windows users) curl -sLSf https://inletsctl.inlets.dev | sh # Install directly to /usr/local/bin/ curl -sLSf https://inletsctl.inlets.dev | sudo sh Windows users are encouraged to use git bash to install the inletsctl binary. Downloading inlets-pro \u00b6 The inletsctl download command can be used to download the inlets/-pro binaries. Example usage: # Download the latest inlets-pro binary inletsctl download # Download a specific version of inlets-pro inletsctl download --version 0 .8.5 The create command \u00b6 Create a HTTPS tunnel with a custom domain \u00b6 This example uses DigitalOcean to create a cloud VM and then exposes a local service via the newly created exit-server. Let's say we want to expose a Grafana server on our internal network to the Internet via Let's Encrypt and HTTPS? export DOMAIN = \"grafana.example.com\" inletsctl create \\ --provider digitalocean \\ --region = \"lon1\" \\ --access-token-file $HOME /do-access-token \\ --letsencrypt-domain $DOMAIN \\ --letsencrypt-email webmaster@ $DOMAIN \\ --letsencrypt-issuer prod You can also use --letsencrypt-issuer with the staging value whilst testing since Let's Encrypt rate-limits how many certificates you can obtain within a week. Create a DNS A record for the IP address so that grafana.example.com for instance resolves to that IP. For instance you could run: doctl compute domain create \\ --ip-address 46 .101.60.161 grafana.example.com Now run the command that you were given, and if you wish, change the upstream to point to the domain explicitly: # Obtain a license at https://inlets.dev # Store it at $HOME/.inlets/LICENSE or use --help for more options # Where to route traffic from the inlets server export UPSTREAM = \"grafana.example.com=http://192.168.0.100:3000\" inlets-pro http client --url \"wss://46.101.60.161:8123\" \\ --token \"lRdRELPrkhA0kxwY0eWoaviWvOoYG0tj212d7Ff0zEVgpnAfh5WjygUVVcZ8xJRJ\" \\ --upstream $UPSTREAM To delete: inletsctl delete --provider digitalocean --id \"248562460\" You can also specify more than one domain and upstream for the same tunnel, so you could expose OpenFaaS and Grafana separately for instance. Update the inletsctl create command with multiple domains such as: --letsencrypt-domain openfaas.example.com --letsencrypt-domain grafana.example.com Then for the inlets-pro client command, update the upstream in the same way by repeating the flag once per upstream mapping: --upstream openfaas.example.com=http://127.0.0.1:8080 --upstream grafana.example.com=http://192.168.0.100:3000 . Note that in previous inlets versions, multiple upstream values were given in a single flag, separated by commas, this has now been deprecated for the above syntax. Create a HTTP tunnel \u00b6 This example uses Linode to create a cloud VM and then exposes a local service via the newly created exit-server. export REGION = \"eu-west\" inletsctl create \\ --provider linode \\ --region = \" $REGION \" \\ --access-token-file $HOME /do-access-token You'll see the host being provisioned, it usually takes just a few seconds: Using provider: linode Requesting host: peaceful-lewin8 in eu-west, from linode 2021/06/01 15:56:03 Provisioning host with Linode Host: 248561704, status: [1/500] Host: 248561704, status: new ... [11/500] Host: 248561704, status: active inlets PRO (0.7.0) exit-server summary: IP: 188.166.168.90 Auth-token: dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI Now run the command given to you, changing the --upstream URL to match a local URL such as http://localhost:3000 # Obtain a license at https://inlets.dev export LICENSE = \" $HOME /.inlets/license\" # Give a single value or comma-separated export PORTS = \"3000\" # Where to route traffic from the inlets server export UPSTREAM = \"localhost\" inlets-pro client --url \"wss://188.166.168.90:8123/connect\" \\ --token \"dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI\" \\ --upstream $UPSTREAM \\ --ports $PORTS The client will look for your license in $HOME/.inlets/LICENSE , but you can also use the --license/--license-file flag if you wish. You can then access your local website via the Internet and the exit-server's IP at: http://188.166.168.90 When you're done, you can delete the host using its ID or IP address: inletsctl delete --provider linode --id \"248561704\" inletsctl delete --provider linode --ip \"188.166.168.90\" Create a tunnel for a TCP service \u00b6 This example is similar to the previous one, but also adds link-level encryption between your local service and the exit-server. In addition, you can also expose pure TCP traffic such as SSH or Postgresql. inletsctl create \\ --provider digitalocean \\ --access-token-file $HOME /do-access-token \\ --pro Note the output: inlets PRO ( 0 .7.0 ) exit-server summary: IP: 142 .93.34.79 Auth-token: TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb Command: export LICENSE = \"\" export PORTS = \"8000\" export UPSTREAM = \"localhost\" inlets-pro client --url \"wss://142.93.34.79:8123/connect\" \\ --token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\ --license \" $LICENSE \" \\ --upstream $UPSTREAM \\ --ports $PORTS To Delete: inletsctl delete --provider digitalocean --id \"205463570\" Run a local service that uses TCP such as MariaDB: head -c 16 /dev/urandom | shasum 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" docker run --name mariadb \\ -p 3306 :3306 \\ -e MYSQL_ROOT_PASSWORD = 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 \\ -ti mariadb:latest Connect to the tunnel updating the ports to 3306 export LICENSE = \" $( cat ~/LICENSE ) \" export PORTS = \"3306\" export UPSTREAM = \"localhost\" inlets-pro client --url \"wss://142.93.34.79:8123/connect\" \\ --token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\ --license \" $LICENSE \" \\ --upstream $UPSTREAM \\ --ports $PORTS Now connect to your MariaDB instance from its public IP address: export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" export EXIT_IP = \"142.93.34.79\" docker run -it --rm mariadb:latest mysql -h $EXIT_IP -P 3306 -uroot -p $PASSWORD Welcome to the MariaDB monitor. Commands end with ; or \\g . Your MariaDB connection id is 3 Server version: 10 .5.5-MariaDB-1:10.5.5+maria~focal mariadb.org binary distribution Copyright ( c ) 2000 , 2018 , Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [( none )] > create database test ; Query OK, 1 row affected ( 0 .039 sec ) Examples for specific cloud providers \u00b6 Example usage with AWS EC2 \u00b6 To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles. Create a AWS IAM Policy with the following: Create a file named policy.json with the following content { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:DescribeInstances\" , \"ec2:DescribeImages\" , \"ec2:TerminateInstances\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateTags\" , \"ec2:DeleteSecurityGroup\" , \"ec2:RunInstances\" , \"ec2:DescribeInstanceStatus\" ], \"Resource\" : [ \"*\" ] } ] } Create the policy in AWS aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json Create an IAM user aws iam create-user --user-name inlets-automation Add the Policy to the IAM user We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below. export AWS_ACCOUNT_NUMBER = \"Your AWS Account Number\" aws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam:: ${ AWS_ACCOUNT_NUMBER } :policy/inlets-automation Generate an access key for your IAM User The below commands will create a set of credentials and save them into files for use later on. we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually. ACCESS_KEY_JSON = $( aws iam create-access-key --user-name inlets-automation ) echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId > access-key.txt echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey > secret-key.txt Create an exit-server: inletsctl create \\ --provider ec2 \\ --region eu-west-1 \\ --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt Delete an exit-server: export IP = \"\" inletsctl create \\ --provider ec2 \\ --region eu-west-1 \\ --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt \\ --ip $IP Example usage with AWS EC2 Temporary Credentials \u00b6 To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles. The following instructions use get-session-token to illustrate the concept. However, it is expected that real world usage would more likely make use of assume-role to obtain temporary credentials. Create a AWS IAM Policy with the following: Create a file named policy.json with the following content { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:DescribeInstances\" , \"ec2:DescribeImages\" , \"ec2:TerminateInstances\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateTags\" , \"ec2:DeleteSecurityGroup\" , \"ec2:RunInstances\" , \"ec2:DescribeInstanceStatus\" ], \"Resource\" : [ \"*\" ] } ] } Create the policy in AWS aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json Create an IAM user aws iam create-user --user-name inlets-automation Add the Policy to the IAM user We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below. export AWS_ACCOUNT_NUMBER = \"Your AWS Account Number\" aws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam:: ${ AWS_ACCOUNT_NUMBER } :policy/inlets-automation Generate an access key for your IAM User The below commands will create a set of credentials and save them into files for use later on. we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually. ACCESS_KEY_JSON = $( aws iam create-access-key --user-name inlets-automation ) export AWS_ACCESS_KEY_ID = $( echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId ) export AWS_SECRET_ACCESS_KEY = $( echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey ) Check that calls are now being executed by the inlets-automation IAM User. aws sts get-caller-identity Ask STS for some temporary credentials TEMP_CREDS = $( aws sts get-session-token ) Break out the required elements echo $TEMP_CREDS | jq -r .Credentials.AccessKeyId > access-key.txt echo $TEMP_CREDS | jq -r .Credentials.SecretAccessKey > secret-key.txt echo $TEMP_CREDS | jq -r .Credentials.SessionToken > session-token.txt Create an exit-server using temporary credentials: inletsctl create \\ --provider ec2 \\ --region eu-west-1 \\ --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt \\ --session-token-file ./session-token.txt Delete an exit-server using temporary credentials: export INSTANCEID = \"\" inletsctl delete \\ --provider ec2 \\ --id $INSTANCEID --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt \\ --session-token-file ./session-token.txt Example usage with Google Compute Engine \u00b6 One time setup required for a service account key It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here # Get current projectID export PROJECTID = $( gcloud config get-value core/project 2 >/dev/null ) # Create a service account gcloud iam service-accounts create inlets \\ --description \"inlets-operator service account\" \\ --display-name \"inlets\" # Get service account email export SERVICEACCOUNT = $( gcloud iam service-accounts list | grep inlets | awk '{print $2}' ) # Assign appropriate roles to inlets service account gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/compute.admin gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/iam.serviceAccountUser # Create inlets service account key file gcloud iam service-accounts keys create key.json \\ --iam-account $SERVICEACCOUNT Run inlets OSS or inlets-pro # Create a tunnel with inlets OSS inletsctl create -p gce --project-id = $PROJECTID -f = key.json ## Create a TCP tunnel with inlets-pro inletsctl create -p gce -p $PROJECTID -f = key.json # Or specify any valid Google Cloud Zone optional zone, by default it get provisioned in us-central1-a inletsctl create -p gce --project-id = $PROJECTID -f key.json --zone = us-central1-a Example usage with Azure \u00b6 Prerequisites: You will need az . See Install the Azure CLI Generate Azure auth file az ad sp create-for-rbac --sdk-auth > ~/Downloads/client_credentials.json Create inletsctl create --provider = azure --subscription-id = 4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\ --region = eastus --access-token-file = ~/Downloads/client_credentials.json Delete inletsctl delete --provider = azure --id inlets-clever-volhard8 \\ --subscription-id = 4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\ --region = eastus --access-token-file = ~/Downloads/client_credentials.json Example usage with Hetzner \u00b6 # Obtain the API token from Hetzner Cloud Console. export TOKEN = \"\" inletsctl create --provider hetzner \\ --access-token $TOKEN \\ --region hel1 Available regions are hel1 (Helsinki), nur1 (Nuremberg), fsn1 (Falkenstein). Example usage with Linode \u00b6 Prerequisites: Prepare a Linode API Access Token. See Create Linode API Access token Create inletsctl create --provider = linode --access-token = <API Access Token> --region = us-east Delete inletsctl delete --provider = linode --access-token = <API Access Token> --id <instance id> Example usage with Scaleway \u00b6 # Obtain from your Scaleway dashboard: export TOKEN = \"\" export SECRET_KEY = \"\" export ORG_ID = \"\" inletsctl create --provider scaleway \\ --access-token $TOKEN --secret-key $SECRET_KEY --organisation-id $ORG_ID The region is hard-coded to France / Paris 1. Example usage with OVHcloud \u00b6 You need to create API keys for the ovhCloud country/continent you're going to deploy with inletsctl. For an overview of available endpoint check supported-apis documentation For, example, Europe visit https://eu.api.ovh.com/createToken to create your API keys. However, the specific value for the endpoint flag are following: ovh-eu for OVH Europe API ovh-us for OVH US API ovh-ca for OVH Canada API soyoustart-eu for So you Start Europe API soyoustart-ca for So you Start Canada API kimsufi-eu for Kimsufi Europe API kimsufi-ca for Kimsufi Canada API ovh-eu is the default endpoint and DE1 the default region. For the proper rights choose all HTTP Verbs (GET,PUT,DELETE, POST), and we need only the /cloud/ API. export APPLICATION_KEY = \"\" export APPLICATION_SECRET = \"\" export CONSUMER_KEY = \"\" export ENDPOINT = \"\" export PROJECT_ID = \"\" inletsctl create --provider ovh \\ --access-token $APPLICATION_KEY \\ --secret-key $APPLICATION_SECRET --consumer-key $CONSUMER_KEY \\ --project-id $SERVICENAME \\ --endpoint $ENDPOINT The delete command \u00b6 The delete command takes an id or IP address which are given to you at the end of the inletsctl create command. You'll also need to specify your cloud access token. inletsctl delete \\ --provider digitalocean \\ --access-token-file ~/Downloads/do-access-token \\ --id 164857028 \\ Or delete via IP: inletsctl delete \\ --provider digitalocean \\ --access-token-file ~/Downloads/do-access-token \\ --ip 209 .97.131.180 \\ kfwd - Kubernetes service forwarding \u00b6 kfwd runs an inlets-pro server on your local computer, then deploys an inlets client in your Kubernetes cluster using a Pod. This enables your local computer to access services from within the cluster as if they were running on your laptop. inlets PRO allows you to access any TCP service within the cluster, using an encrypted link: Forward the figlet pod from openfaas-fn on port 8080 : inletsctl kfwd \\ --pro \\ --license $( cat ~/LICENSE ) --from figlet:8080 \\ --namespace openfaas-fn \\ --if 192 .168.0.14 Note the if parameter is the IP address of your local computer, this must be reachable from the Kubernetes cluster. Then access the service via http://127.0.0.1:8080 . Troubleshooting \u00b6 inletsctl provisions a host called an exit node or exit server using public cloud APIs. It then prints out a connection string. Are you unable to connect your client to the exit server? Troubleshooting inlets PRO \u00b6 If using auto-tls (the default), check that port 8123 is accessible. It should be serving a file with a self-signed certificate, run the following: export IP = 192 .168.0.1 curl -k https:// $IP :8123/.well-known/ca.crt If you see connection refused, log in to the host over SSH and check the service via systemctl: sudo systemctl status inlets-pro # Check its logs sudo journalctl -u inlets-pro You can also check the configuration in /etc/default/inlets-pro , to make sure that an IP address and token are configured. Configuration using environment variables \u00b6 You may want to set an environment variable that points at your access-token-file or secret-key-file Inlets will look for the following: # For providers that use --access-token-file INLETS_ACCESS_TOKEN # For providers that use --secret-key-file INLETS_SECRET_KEY With the correct one of these set you wont need to add the flag on every command execution. You can set the following syntax in your bashrc (or equivalent for your shell) export INLETS_ACCESS_TOKEN = $( cat my-token.txt ) # or set the INLETS_SECRET_KEY for those providors that use this export INLETS_SECRET_KEY = $( cat my-token.txt )","title":"inletsctl reference documentation"},{"location":"reference/inletsctl/#inletsctl-reference-documentation","text":"inletsctl is an automation tool for inlets/-pro. Features: * create / delete cloud VMs with inlets/-pro pre-installed via systemd * download [--pro] - download the inlets/-pro binaries to your local computer * kfwd - forward services from a Kubernetes cluster to your local machine using inlets/-pro View the code on GitHub: inlets/inletsctl","title":"inletsctl reference documentation"},{"location":"reference/inletsctl/#install-inletsctl","text":"You can install inletsctl using its installer, or from the GitHub releases page # Install to local directory (and for Windows users) curl -sLSf https://inletsctl.inlets.dev | sh # Install directly to /usr/local/bin/ curl -sLSf https://inletsctl.inlets.dev | sudo sh Windows users are encouraged to use git bash to install the inletsctl binary.","title":"Install inletsctl"},{"location":"reference/inletsctl/#downloading-inlets-pro","text":"The inletsctl download command can be used to download the inlets/-pro binaries. Example usage: # Download the latest inlets-pro binary inletsctl download # Download a specific version of inlets-pro inletsctl download --version 0 .8.5","title":"Downloading inlets-pro"},{"location":"reference/inletsctl/#the-create-command","text":"","title":"The create command"},{"location":"reference/inletsctl/#create-a-https-tunnel-with-a-custom-domain","text":"This example uses DigitalOcean to create a cloud VM and then exposes a local service via the newly created exit-server. Let's say we want to expose a Grafana server on our internal network to the Internet via Let's Encrypt and HTTPS? export DOMAIN = \"grafana.example.com\" inletsctl create \\ --provider digitalocean \\ --region = \"lon1\" \\ --access-token-file $HOME /do-access-token \\ --letsencrypt-domain $DOMAIN \\ --letsencrypt-email webmaster@ $DOMAIN \\ --letsencrypt-issuer prod You can also use --letsencrypt-issuer with the staging value whilst testing since Let's Encrypt rate-limits how many certificates you can obtain within a week. Create a DNS A record for the IP address so that grafana.example.com for instance resolves to that IP. For instance you could run: doctl compute domain create \\ --ip-address 46 .101.60.161 grafana.example.com Now run the command that you were given, and if you wish, change the upstream to point to the domain explicitly: # Obtain a license at https://inlets.dev # Store it at $HOME/.inlets/LICENSE or use --help for more options # Where to route traffic from the inlets server export UPSTREAM = \"grafana.example.com=http://192.168.0.100:3000\" inlets-pro http client --url \"wss://46.101.60.161:8123\" \\ --token \"lRdRELPrkhA0kxwY0eWoaviWvOoYG0tj212d7Ff0zEVgpnAfh5WjygUVVcZ8xJRJ\" \\ --upstream $UPSTREAM To delete: inletsctl delete --provider digitalocean --id \"248562460\" You can also specify more than one domain and upstream for the same tunnel, so you could expose OpenFaaS and Grafana separately for instance. Update the inletsctl create command with multiple domains such as: --letsencrypt-domain openfaas.example.com --letsencrypt-domain grafana.example.com Then for the inlets-pro client command, update the upstream in the same way by repeating the flag once per upstream mapping: --upstream openfaas.example.com=http://127.0.0.1:8080 --upstream grafana.example.com=http://192.168.0.100:3000 . Note that in previous inlets versions, multiple upstream values were given in a single flag, separated by commas, this has now been deprecated for the above syntax.","title":"Create a HTTPS tunnel with a custom domain"},{"location":"reference/inletsctl/#create-a-http-tunnel","text":"This example uses Linode to create a cloud VM and then exposes a local service via the newly created exit-server. export REGION = \"eu-west\" inletsctl create \\ --provider linode \\ --region = \" $REGION \" \\ --access-token-file $HOME /do-access-token You'll see the host being provisioned, it usually takes just a few seconds: Using provider: linode Requesting host: peaceful-lewin8 in eu-west, from linode 2021/06/01 15:56:03 Provisioning host with Linode Host: 248561704, status: [1/500] Host: 248561704, status: new ... [11/500] Host: 248561704, status: active inlets PRO (0.7.0) exit-server summary: IP: 188.166.168.90 Auth-token: dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI Now run the command given to you, changing the --upstream URL to match a local URL such as http://localhost:3000 # Obtain a license at https://inlets.dev export LICENSE = \" $HOME /.inlets/license\" # Give a single value or comma-separated export PORTS = \"3000\" # Where to route traffic from the inlets server export UPSTREAM = \"localhost\" inlets-pro client --url \"wss://188.166.168.90:8123/connect\" \\ --token \"dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI\" \\ --upstream $UPSTREAM \\ --ports $PORTS The client will look for your license in $HOME/.inlets/LICENSE , but you can also use the --license/--license-file flag if you wish. You can then access your local website via the Internet and the exit-server's IP at: http://188.166.168.90 When you're done, you can delete the host using its ID or IP address: inletsctl delete --provider linode --id \"248561704\" inletsctl delete --provider linode --ip \"188.166.168.90\"","title":"Create a HTTP tunnel"},{"location":"reference/inletsctl/#create-a-tunnel-for-a-tcp-service","text":"This example is similar to the previous one, but also adds link-level encryption between your local service and the exit-server. In addition, you can also expose pure TCP traffic such as SSH or Postgresql. inletsctl create \\ --provider digitalocean \\ --access-token-file $HOME /do-access-token \\ --pro Note the output: inlets PRO ( 0 .7.0 ) exit-server summary: IP: 142 .93.34.79 Auth-token: TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb Command: export LICENSE = \"\" export PORTS = \"8000\" export UPSTREAM = \"localhost\" inlets-pro client --url \"wss://142.93.34.79:8123/connect\" \\ --token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\ --license \" $LICENSE \" \\ --upstream $UPSTREAM \\ --ports $PORTS To Delete: inletsctl delete --provider digitalocean --id \"205463570\" Run a local service that uses TCP such as MariaDB: head -c 16 /dev/urandom | shasum 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" docker run --name mariadb \\ -p 3306 :3306 \\ -e MYSQL_ROOT_PASSWORD = 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 \\ -ti mariadb:latest Connect to the tunnel updating the ports to 3306 export LICENSE = \" $( cat ~/LICENSE ) \" export PORTS = \"3306\" export UPSTREAM = \"localhost\" inlets-pro client --url \"wss://142.93.34.79:8123/connect\" \\ --token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\ --license \" $LICENSE \" \\ --upstream $UPSTREAM \\ --ports $PORTS Now connect to your MariaDB instance from its public IP address: export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" export EXIT_IP = \"142.93.34.79\" docker run -it --rm mariadb:latest mysql -h $EXIT_IP -P 3306 -uroot -p $PASSWORD Welcome to the MariaDB monitor. Commands end with ; or \\g . Your MariaDB connection id is 3 Server version: 10 .5.5-MariaDB-1:10.5.5+maria~focal mariadb.org binary distribution Copyright ( c ) 2000 , 2018 , Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [( none )] > create database test ; Query OK, 1 row affected ( 0 .039 sec )","title":"Create a tunnel for a TCP service"},{"location":"reference/inletsctl/#examples-for-specific-cloud-providers","text":"","title":"Examples for specific cloud providers"},{"location":"reference/inletsctl/#example-usage-with-aws-ec2","text":"To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles. Create a AWS IAM Policy with the following: Create a file named policy.json with the following content { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:DescribeInstances\" , \"ec2:DescribeImages\" , \"ec2:TerminateInstances\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateTags\" , \"ec2:DeleteSecurityGroup\" , \"ec2:RunInstances\" , \"ec2:DescribeInstanceStatus\" ], \"Resource\" : [ \"*\" ] } ] } Create the policy in AWS aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json Create an IAM user aws iam create-user --user-name inlets-automation Add the Policy to the IAM user We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below. export AWS_ACCOUNT_NUMBER = \"Your AWS Account Number\" aws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam:: ${ AWS_ACCOUNT_NUMBER } :policy/inlets-automation Generate an access key for your IAM User The below commands will create a set of credentials and save them into files for use later on. we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually. ACCESS_KEY_JSON = $( aws iam create-access-key --user-name inlets-automation ) echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId > access-key.txt echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey > secret-key.txt Create an exit-server: inletsctl create \\ --provider ec2 \\ --region eu-west-1 \\ --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt Delete an exit-server: export IP = \"\" inletsctl create \\ --provider ec2 \\ --region eu-west-1 \\ --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt \\ --ip $IP","title":"Example usage with AWS EC2"},{"location":"reference/inletsctl/#example-usage-with-aws-ec2-temporary-credentials","text":"To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles. The following instructions use get-session-token to illustrate the concept. However, it is expected that real world usage would more likely make use of assume-role to obtain temporary credentials. Create a AWS IAM Policy with the following: Create a file named policy.json with the following content { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:DescribeInstances\" , \"ec2:DescribeImages\" , \"ec2:TerminateInstances\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateTags\" , \"ec2:DeleteSecurityGroup\" , \"ec2:RunInstances\" , \"ec2:DescribeInstanceStatus\" ], \"Resource\" : [ \"*\" ] } ] } Create the policy in AWS aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json Create an IAM user aws iam create-user --user-name inlets-automation Add the Policy to the IAM user We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below. export AWS_ACCOUNT_NUMBER = \"Your AWS Account Number\" aws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam:: ${ AWS_ACCOUNT_NUMBER } :policy/inlets-automation Generate an access key for your IAM User The below commands will create a set of credentials and save them into files for use later on. we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually. ACCESS_KEY_JSON = $( aws iam create-access-key --user-name inlets-automation ) export AWS_ACCESS_KEY_ID = $( echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId ) export AWS_SECRET_ACCESS_KEY = $( echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey ) Check that calls are now being executed by the inlets-automation IAM User. aws sts get-caller-identity Ask STS for some temporary credentials TEMP_CREDS = $( aws sts get-session-token ) Break out the required elements echo $TEMP_CREDS | jq -r .Credentials.AccessKeyId > access-key.txt echo $TEMP_CREDS | jq -r .Credentials.SecretAccessKey > secret-key.txt echo $TEMP_CREDS | jq -r .Credentials.SessionToken > session-token.txt Create an exit-server using temporary credentials: inletsctl create \\ --provider ec2 \\ --region eu-west-1 \\ --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt \\ --session-token-file ./session-token.txt Delete an exit-server using temporary credentials: export INSTANCEID = \"\" inletsctl delete \\ --provider ec2 \\ --id $INSTANCEID --access-token-file ./access-key.txt \\ --secret-key-file ./secret-key.txt \\ --session-token-file ./session-token.txt","title":"Example usage with AWS EC2 Temporary Credentials"},{"location":"reference/inletsctl/#example-usage-with-google-compute-engine","text":"One time setup required for a service account key It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here # Get current projectID export PROJECTID = $( gcloud config get-value core/project 2 >/dev/null ) # Create a service account gcloud iam service-accounts create inlets \\ --description \"inlets-operator service account\" \\ --display-name \"inlets\" # Get service account email export SERVICEACCOUNT = $( gcloud iam service-accounts list | grep inlets | awk '{print $2}' ) # Assign appropriate roles to inlets service account gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/compute.admin gcloud projects add-iam-policy-binding $PROJECTID \\ --member serviceAccount: $SERVICEACCOUNT \\ --role roles/iam.serviceAccountUser # Create inlets service account key file gcloud iam service-accounts keys create key.json \\ --iam-account $SERVICEACCOUNT Run inlets OSS or inlets-pro # Create a tunnel with inlets OSS inletsctl create -p gce --project-id = $PROJECTID -f = key.json ## Create a TCP tunnel with inlets-pro inletsctl create -p gce -p $PROJECTID -f = key.json # Or specify any valid Google Cloud Zone optional zone, by default it get provisioned in us-central1-a inletsctl create -p gce --project-id = $PROJECTID -f key.json --zone = us-central1-a","title":"Example usage with Google Compute Engine"},{"location":"reference/inletsctl/#example-usage-with-azure","text":"Prerequisites: You will need az . See Install the Azure CLI Generate Azure auth file az ad sp create-for-rbac --sdk-auth > ~/Downloads/client_credentials.json Create inletsctl create --provider = azure --subscription-id = 4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\ --region = eastus --access-token-file = ~/Downloads/client_credentials.json Delete inletsctl delete --provider = azure --id inlets-clever-volhard8 \\ --subscription-id = 4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\ --region = eastus --access-token-file = ~/Downloads/client_credentials.json","title":"Example usage with Azure"},{"location":"reference/inletsctl/#example-usage-with-hetzner","text":"# Obtain the API token from Hetzner Cloud Console. export TOKEN = \"\" inletsctl create --provider hetzner \\ --access-token $TOKEN \\ --region hel1 Available regions are hel1 (Helsinki), nur1 (Nuremberg), fsn1 (Falkenstein).","title":"Example usage with Hetzner"},{"location":"reference/inletsctl/#example-usage-with-linode","text":"Prerequisites: Prepare a Linode API Access Token. See Create Linode API Access token Create inletsctl create --provider = linode --access-token = <API Access Token> --region = us-east Delete inletsctl delete --provider = linode --access-token = <API Access Token> --id <instance id>","title":"Example usage with Linode"},{"location":"reference/inletsctl/#example-usage-with-scaleway","text":"# Obtain from your Scaleway dashboard: export TOKEN = \"\" export SECRET_KEY = \"\" export ORG_ID = \"\" inletsctl create --provider scaleway \\ --access-token $TOKEN --secret-key $SECRET_KEY --organisation-id $ORG_ID The region is hard-coded to France / Paris 1.","title":"Example usage with Scaleway"},{"location":"reference/inletsctl/#example-usage-with-ovhcloud","text":"You need to create API keys for the ovhCloud country/continent you're going to deploy with inletsctl. For an overview of available endpoint check supported-apis documentation For, example, Europe visit https://eu.api.ovh.com/createToken to create your API keys. However, the specific value for the endpoint flag are following: ovh-eu for OVH Europe API ovh-us for OVH US API ovh-ca for OVH Canada API soyoustart-eu for So you Start Europe API soyoustart-ca for So you Start Canada API kimsufi-eu for Kimsufi Europe API kimsufi-ca for Kimsufi Canada API ovh-eu is the default endpoint and DE1 the default region. For the proper rights choose all HTTP Verbs (GET,PUT,DELETE, POST), and we need only the /cloud/ API. export APPLICATION_KEY = \"\" export APPLICATION_SECRET = \"\" export CONSUMER_KEY = \"\" export ENDPOINT = \"\" export PROJECT_ID = \"\" inletsctl create --provider ovh \\ --access-token $APPLICATION_KEY \\ --secret-key $APPLICATION_SECRET --consumer-key $CONSUMER_KEY \\ --project-id $SERVICENAME \\ --endpoint $ENDPOINT","title":"Example usage with OVHcloud"},{"location":"reference/inletsctl/#the-delete-command","text":"The delete command takes an id or IP address which are given to you at the end of the inletsctl create command. You'll also need to specify your cloud access token. inletsctl delete \\ --provider digitalocean \\ --access-token-file ~/Downloads/do-access-token \\ --id 164857028 \\ Or delete via IP: inletsctl delete \\ --provider digitalocean \\ --access-token-file ~/Downloads/do-access-token \\ --ip 209 .97.131.180 \\","title":"The delete command"},{"location":"reference/inletsctl/#kfwd-kubernetes-service-forwarding","text":"kfwd runs an inlets-pro server on your local computer, then deploys an inlets client in your Kubernetes cluster using a Pod. This enables your local computer to access services from within the cluster as if they were running on your laptop. inlets PRO allows you to access any TCP service within the cluster, using an encrypted link: Forward the figlet pod from openfaas-fn on port 8080 : inletsctl kfwd \\ --pro \\ --license $( cat ~/LICENSE ) --from figlet:8080 \\ --namespace openfaas-fn \\ --if 192 .168.0.14 Note the if parameter is the IP address of your local computer, this must be reachable from the Kubernetes cluster. Then access the service via http://127.0.0.1:8080 .","title":"kfwd - Kubernetes service forwarding"},{"location":"reference/inletsctl/#troubleshooting","text":"inletsctl provisions a host called an exit node or exit server using public cloud APIs. It then prints out a connection string. Are you unable to connect your client to the exit server?","title":"Troubleshooting"},{"location":"reference/inletsctl/#troubleshooting-inlets-pro","text":"If using auto-tls (the default), check that port 8123 is accessible. It should be serving a file with a self-signed certificate, run the following: export IP = 192 .168.0.1 curl -k https:// $IP :8123/.well-known/ca.crt If you see connection refused, log in to the host over SSH and check the service via systemctl: sudo systemctl status inlets-pro # Check its logs sudo journalctl -u inlets-pro You can also check the configuration in /etc/default/inlets-pro , to make sure that an IP address and token are configured.","title":"Troubleshooting inlets PRO"},{"location":"reference/inletsctl/#configuration-using-environment-variables","text":"You may want to set an environment variable that points at your access-token-file or secret-key-file Inlets will look for the following: # For providers that use --access-token-file INLETS_ACCESS_TOKEN # For providers that use --secret-key-file INLETS_SECRET_KEY With the correct one of these set you wont need to add the flag on every command execution. You can set the following syntax in your bashrc (or equivalent for your shell) export INLETS_ACCESS_TOKEN = $( cat my-token.txt ) # or set the INLETS_SECRET_KEY for those providors that use this export INLETS_SECRET_KEY = $( cat my-token.txt )","title":"Configuration using environment variables"},{"location":"tutorial/automated-http-server/","text":"Automated HTTP tunnel server \u00b6 Learn how to serve traffic from your private network over a private tunnel server. At the end of this tutorial, you'll have a a secure TLS public endpoint using your own DNS and domain, which you can use to access your internal services or webpages. I'll show you how to: automate a tunnel server on a public cloud provider with inlets pre-loaded onto it, how to connect a client from your home or private network how to tunnel one or more services and what else you can do In a previous article , I explained some of the differences between SaaS and private tunnel servers. Create your tunnel server \u00b6 With SaaS tunnels, your tunnels server processes run on shared servers with other users. With a private tunnel server like inlets, you need to create a server somewhere on the Internet to run the tunnel. It should be created with a public IP address that you can use to accept traffic and proxy it into your private network. Pictured: Inlets Conceptual architecture The simplest way to do this is to use the inletsctl tool, which supports around a dozen clouds. The alternative is to set up a VPS or install inlets-pro onto a server you already have set up, and then add a systemd unit file so that it restarts if the tunnel or server should crash for any reason. To see a list of supported clouds run: inletsctl create --help For instructions on how to create an API key or service account for each, feel free to browse the docs . inletsctl create \\ --region lon1 \\ --provider digitalocean \\ --access-token-file ~/digital-ocean-api-key.txt \\ --letsencrypt-domain blog.example.com \\ --letsencrypt-email webmaster@example.com A VM will be created in your account using the cheapest plan available, for DigitalOcean this costs 5 USD / mo at time of writing. You can also run your tunnel server in the free tier of GCP, Oracle Cloud or on Fly.io at no additional cost. Once the tunnel server has been created, you will receive: The IP address An endpoint for the inlets client to connect to A token for the inlets client to use when connecting Take a note of these. Now create a DNS \"A\" record for the IP address of the tunnel server on your domain control panel. Personally, I'm a fan of Google Domains and the .dev domains, but DigitalOcean can also manage domains through their CLI: export IP = \"\" export SUBDOMAIN = \"blog.example.com\" doctl compute domain create $SUBDOMAIN \\ --ip-address $IP How does the TLS encryption work? The inlets server process will attempt to get a TLS certificate from Let's Encrypt using a HTTP01 Acme challenge. What if I have multiple sites? You can pass a number of sub-domains, for instance: --letsencrypt-domain blog.example.com,grafana.example.com \\ --letsencrypt-email webmaster@example.com Connect your tunnel client \u00b6 The tunnel client can be run as and when required, or you can generate a systemd unit file so that you can have it running in the background. You can run the tunnel on the same machine as the service that you're proxying, or you can run it on another computer. It's entirely up to you. So you could have a Raspberry Pi which just runs Raspberry Pi OS Lite and an inlets client, and nothing else. In this way you're creating a kind of router appliance . Let's imagine you've run a Node.js express service on your computer: $ git clone https://github.com/alexellis/alexellis.io \\ --depth = 1 $ cd alexellis.io/ $ npm install $ npm start alexellis.io started on port: http://0.0.0.0:3000 inlets also has its own built-in file-server with password protection and the ability to disable browsing for sharing private links. You can expose the built-in file-server when you want to share files directly, without having to upload them first: The simple way to share files directly from your computer You can download the inlets client using the inletsctl tool: $ sudo inletsctl download Now you can start the tunnel client and start serving a test version of my personal homepage alexellis.io : $ export URL = \"\" $ export TOKEN = \"\" $ inlets-pro http client \\ --url $URL \\ --token $TOKEN \\ --upstream blog.example.com = http://127.0.0.1:3000 What if my services are running on different computers? If they are all within the same network, then you can run the client in one place and have it point at the various internal IP addresses. $ inlets-pro http client \\ --url $URL \\ --token $TOKEN \\ --upstream blog.example.com = http://127.0.0.1:3000 \\ --upstream grafana.example.com = http://192.168.0.100:3000 If they are on different networks, you can simply run multiple clients, just change the --upstream flag on each client. How can I run the client in the background? For Linux hosts, you can generate a systemd unit file for inlets by using the --generate systemd flag to the client or server command. Then simply copy the resulting file to the correct location on your system and install it: $ export URL = \"\" $ export TOKEN = \"\" $ inlets-pro http client \\ --url $URL \\ --token $TOKEN \\ --upstream blog.example.com = http://127.0.0.1:3000 \\ --generate = systemd > inlets.service $ sudo cp inlets.service /etc/systemd/system/ $ sudo systemctl enable inlets You can then check the logs or service status: $ sudo journalctl -u inlets $ sudo systemctl status inlets Access your website over the tunnel \u00b6 You can now access your local website being served at http://127.0.0.1:3000 over the tunnel by visiting the domain you created: https://blog.example.com/ Your IP goes where you go \u00b6 You can close the lid on your laptop, and open it again in Starbucks or your favourite local independent coffee shop. As soon as you reconnect the client, your local server will be available over the tunnel at the same IP address and domain: https://blog.example.com/ I used this technique to test a live demo for the KubeCon conference. I then took a flight from London to San Diego and was able to receive traffic to my Raspberry Pi whilst tethering on a local SIM card. Tethering my Raspberry Pi with K3s in San Diego Wrapping up \u00b6 In a very short period of time we created a private tunnel server on a public cloud of our choice, then we created a DNS record for it, and connected a client and accessed our local website. You can get started with inlets through a monthly subscription , or save on a yearly plan. When would you need this? If you're self-hosting websites, you already have some equipment at home, so it can work out cheaper. If you're running a Kubernetes cluster or K3s on a Raspberry Pi, it can be much cheaper over the course of a year. But it's also incredibly convenient for sharing files and for testing APIs or OAuth flows during development. Ben Potter at Coder is writing up a tutorial on how to access a private VSCode server from anywhere using a private tunnel. If you would like to learn more, follow @inletsdev for when it gets published. Andrew Meier put it this way: \"I prefer to play around with different projects without having to worry about my costs skyrocketing. I had a few Raspberry Pis and wondered if I could use them as a cluster. After a bit of searching #k3s and inlets gave me my answer\" Andrew's K3s cluster, with inlets Read his blog post: Personal Infrastructure with Inlets, k3s, and Pulumi You may also like \u00b6 Tunnel a service or ingress from Kubernetes Share a file without uploading it through inlets tunnels Connecting my boat to the Internet with inlets","title":"Automated http server"},{"location":"tutorial/automated-http-server/#automated-http-tunnel-server","text":"Learn how to serve traffic from your private network over a private tunnel server. At the end of this tutorial, you'll have a a secure TLS public endpoint using your own DNS and domain, which you can use to access your internal services or webpages. I'll show you how to: automate a tunnel server on a public cloud provider with inlets pre-loaded onto it, how to connect a client from your home or private network how to tunnel one or more services and what else you can do In a previous article , I explained some of the differences between SaaS and private tunnel servers.","title":"Automated HTTP tunnel server"},{"location":"tutorial/automated-http-server/#create-your-tunnel-server","text":"With SaaS tunnels, your tunnels server processes run on shared servers with other users. With a private tunnel server like inlets, you need to create a server somewhere on the Internet to run the tunnel. It should be created with a public IP address that you can use to accept traffic and proxy it into your private network. Pictured: Inlets Conceptual architecture The simplest way to do this is to use the inletsctl tool, which supports around a dozen clouds. The alternative is to set up a VPS or install inlets-pro onto a server you already have set up, and then add a systemd unit file so that it restarts if the tunnel or server should crash for any reason. To see a list of supported clouds run: inletsctl create --help For instructions on how to create an API key or service account for each, feel free to browse the docs . inletsctl create \\ --region lon1 \\ --provider digitalocean \\ --access-token-file ~/digital-ocean-api-key.txt \\ --letsencrypt-domain blog.example.com \\ --letsencrypt-email webmaster@example.com A VM will be created in your account using the cheapest plan available, for DigitalOcean this costs 5 USD / mo at time of writing. You can also run your tunnel server in the free tier of GCP, Oracle Cloud or on Fly.io at no additional cost. Once the tunnel server has been created, you will receive: The IP address An endpoint for the inlets client to connect to A token for the inlets client to use when connecting Take a note of these. Now create a DNS \"A\" record for the IP address of the tunnel server on your domain control panel. Personally, I'm a fan of Google Domains and the .dev domains, but DigitalOcean can also manage domains through their CLI: export IP = \"\" export SUBDOMAIN = \"blog.example.com\" doctl compute domain create $SUBDOMAIN \\ --ip-address $IP How does the TLS encryption work? The inlets server process will attempt to get a TLS certificate from Let's Encrypt using a HTTP01 Acme challenge. What if I have multiple sites? You can pass a number of sub-domains, for instance: --letsencrypt-domain blog.example.com,grafana.example.com \\ --letsencrypt-email webmaster@example.com","title":"Create your tunnel server"},{"location":"tutorial/automated-http-server/#connect-your-tunnel-client","text":"The tunnel client can be run as and when required, or you can generate a systemd unit file so that you can have it running in the background. You can run the tunnel on the same machine as the service that you're proxying, or you can run it on another computer. It's entirely up to you. So you could have a Raspberry Pi which just runs Raspberry Pi OS Lite and an inlets client, and nothing else. In this way you're creating a kind of router appliance . Let's imagine you've run a Node.js express service on your computer: $ git clone https://github.com/alexellis/alexellis.io \\ --depth = 1 $ cd alexellis.io/ $ npm install $ npm start alexellis.io started on port: http://0.0.0.0:3000 inlets also has its own built-in file-server with password protection and the ability to disable browsing for sharing private links. You can expose the built-in file-server when you want to share files directly, without having to upload them first: The simple way to share files directly from your computer You can download the inlets client using the inletsctl tool: $ sudo inletsctl download Now you can start the tunnel client and start serving a test version of my personal homepage alexellis.io : $ export URL = \"\" $ export TOKEN = \"\" $ inlets-pro http client \\ --url $URL \\ --token $TOKEN \\ --upstream blog.example.com = http://127.0.0.1:3000 What if my services are running on different computers? If they are all within the same network, then you can run the client in one place and have it point at the various internal IP addresses. $ inlets-pro http client \\ --url $URL \\ --token $TOKEN \\ --upstream blog.example.com = http://127.0.0.1:3000 \\ --upstream grafana.example.com = http://192.168.0.100:3000 If they are on different networks, you can simply run multiple clients, just change the --upstream flag on each client. How can I run the client in the background? For Linux hosts, you can generate a systemd unit file for inlets by using the --generate systemd flag to the client or server command. Then simply copy the resulting file to the correct location on your system and install it: $ export URL = \"\" $ export TOKEN = \"\" $ inlets-pro http client \\ --url $URL \\ --token $TOKEN \\ --upstream blog.example.com = http://127.0.0.1:3000 \\ --generate = systemd > inlets.service $ sudo cp inlets.service /etc/systemd/system/ $ sudo systemctl enable inlets You can then check the logs or service status: $ sudo journalctl -u inlets $ sudo systemctl status inlets","title":"Connect your tunnel client"},{"location":"tutorial/automated-http-server/#access-your-website-over-the-tunnel","text":"You can now access your local website being served at http://127.0.0.1:3000 over the tunnel by visiting the domain you created: https://blog.example.com/","title":"Access your website over the tunnel"},{"location":"tutorial/automated-http-server/#your-ip-goes-where-you-go","text":"You can close the lid on your laptop, and open it again in Starbucks or your favourite local independent coffee shop. As soon as you reconnect the client, your local server will be available over the tunnel at the same IP address and domain: https://blog.example.com/ I used this technique to test a live demo for the KubeCon conference. I then took a flight from London to San Diego and was able to receive traffic to my Raspberry Pi whilst tethering on a local SIM card. Tethering my Raspberry Pi with K3s in San Diego","title":"Your IP goes where you go"},{"location":"tutorial/automated-http-server/#wrapping-up","text":"In a very short period of time we created a private tunnel server on a public cloud of our choice, then we created a DNS record for it, and connected a client and accessed our local website. You can get started with inlets through a monthly subscription , or save on a yearly plan. When would you need this? If you're self-hosting websites, you already have some equipment at home, so it can work out cheaper. If you're running a Kubernetes cluster or K3s on a Raspberry Pi, it can be much cheaper over the course of a year. But it's also incredibly convenient for sharing files and for testing APIs or OAuth flows during development. Ben Potter at Coder is writing up a tutorial on how to access a private VSCode server from anywhere using a private tunnel. If you would like to learn more, follow @inletsdev for when it gets published. Andrew Meier put it this way: \"I prefer to play around with different projects without having to worry about my costs skyrocketing. I had a few Raspberry Pis and wondered if I could use them as a cluster. After a bit of searching #k3s and inlets gave me my answer\" Andrew's K3s cluster, with inlets Read his blog post: Personal Infrastructure with Inlets, k3s, and Pulumi","title":"Wrapping up"},{"location":"tutorial/automated-http-server/#you-may-also-like","text":"Tunnel a service or ingress from Kubernetes Share a file without uploading it through inlets tunnels Connecting my boat to the Internet with inlets","title":"You may also like"},{"location":"tutorial/caddy-http-tunnel/","text":"Custom reverse proxy with Caddy \u00b6 In this tutorial we'll set up an inlets TCP tunnel server to forward ports 80 and 443 to a reverse proxy server running on our local machine. Caddy will receive a TCP stream from the public tunnel server for ports 80 and 443. It can terminate TLS and also allow you to host multiple sites with ease. Caddy is a free and open-source reverse proxy. It's often used on web-servers to add TLS to one or more virtual websites. Pre-reqs \u00b6 A Linux server, Windows and MacOS are also supported The inlets-pro binary at /usr/local/bin/ Access to a DNS control plane for a domain you control You can run through the same instructions with other reverse proxies such as Nginx , or Traefik . Scenario: * You want to share a file such as a VM image or a ISO over the Internet, with HTTPS, directly from your laptop. * You have one or more websites or APIs running on-premises or within your home-lab and want to expose them on the Internet. You can subscribe to inlets for personal or commercial use via Gumroad Setup your exit node \u00b6 Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl : inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --pro Note the --url and TOKEN given to you in this step. Setup your DNS A record \u00b6 Setup a DNS A record for the site you want to expose using the public IP of the cloud VM 178.128.40.109 = service.example.com Run a local server to share files \u00b6 Do not run this command in your home folder. mkdir -p /tmp/shared/ cd /tmp/shared/ echo \"Hello world\" > WELCOME.txt # If using Python 2.x python -m SimpleHTTPServer # Python 3.x python3 -m http.server This will listen on port 8000 by default. Setup Caddy 1.x \u00b6 Download the latest Caddy 1.x binary from the Releases page Pick your operating system, for instance Darwin for MacOS, or Linux. Download the binary, extract it and install it to /usr/local/bin : mkdir -p /tmp/caddy curl -sLSf https://github.com/caddyserver/caddy/releases/download/v1.0.4/caddy_v1.0.4_darwin_amd64.zip > caddy.tar.gz tar -xvf caddy.tar.gz --strip-components = 0 -C /tmp/caddy sudo cp /tmp/caddy/caddy /usr/local/bin/ Create a Caddyfile The Caddyfile configures which websites Caddy will expose, and which sites need a TLS certificate. Replace service.example.com with your own domain. Next, edit proxy / 127.0.0.1:8000 and change the port 8000 to the port of your local webserver, for instance 3000 or 8080 . For our example, keep it as 8000 . service.example.com proxy / 127 .0.0.1:8000 { transparent } Start the Caddy binary, it will listen on port 80 and 443. sudo ./caddy If you have more than one website, you can add them to the Caddyfile on new lines. You'll need to run caddy as sudo so that it can bind to ports 80, and 443 which require additional privileges. Start the inlets-pro client on your local side \u00b6 Downloads the inlets PRO client: sudo inletsctl download Run the inlets-pro client, using the TOKEN and IP given to you from the previous step. The client will look for your license in $HOME/.inlets/LICENSE , but you can also use the --license/--license-file flag if you wish. export IP = \"\" # take this from the exit-server export TOKEN = \"\" # take this from the exit-server inlets-pro client \\ --url wss:// $IP :8123/connect \\ --ports 80 ,443 \\ --token $TOKEN \\ --upstream localhost Note that --upstream localhost will connect to Caddy running on your computer, if you are running Caddy on another machine, use its IP address here. Check it all worked \u00b6 You'll see that Caddy can now obtain a TLS certificate. Go ahead and visit: https://service.example.com Congratulations, you've now served a TLS certificate directly from your laptop. You can close caddy and open it again at a later date. Caddy will re-use the certificate it already obtained and it will be valid for 3 months. To renew, just keep Caddy running or open it again whenever you need it. Setup Caddy 2.x \u00b6 For Caddy 2.x, the Caddyfile format changes. Let's say you're running a Node.js service on port 3000, and want to expose it with TLS on the domain \"service.example.com\": git clone https://github.com/alexellis/expressjs-k8s/ cd expressjs-k8s npm install http_port=3000 npm start The local site will be served at http://127.0.0.1:3000 { acme_ca https://acme-staging-v02.api.letsencrypt.org/directory } service.example.com reverse_proxy 127.0.0.1:3000 { } Note the acme_ca being used will receive a staging certificate, remove it to obtain a production TLS certificate. Now download Caddy 2.x for your operating system. sudo ./caddy run \\ -config ./Caddyfile sudo - is required to bind to port 80 and 443, although you can potentially update your OS to allow binding to low ports without root access. You should now be able to access the Node.js website via the https://service.example.com URL. Caddy also supports multiple domains within the same file, so that you can expose multiple internal or private websites through the same tunnel. { email \"webmaster@example.com\" } blog.example.com { reverse_proxy 127.0.0.1:4000 } openfaas.example.com { reverse_proxy 127.0.0.1:8080 } If you have services running on other machines you can change 127.0.0.1:8080 to a different IP address such as that of your Raspberry Pi if you had something like OpenFaaS running there.","title":"Caddy http tunnel"},{"location":"tutorial/caddy-http-tunnel/#custom-reverse-proxy-with-caddy","text":"In this tutorial we'll set up an inlets TCP tunnel server to forward ports 80 and 443 to a reverse proxy server running on our local machine. Caddy will receive a TCP stream from the public tunnel server for ports 80 and 443. It can terminate TLS and also allow you to host multiple sites with ease. Caddy is a free and open-source reverse proxy. It's often used on web-servers to add TLS to one or more virtual websites.","title":"Custom reverse proxy with Caddy"},{"location":"tutorial/caddy-http-tunnel/#pre-reqs","text":"A Linux server, Windows and MacOS are also supported The inlets-pro binary at /usr/local/bin/ Access to a DNS control plane for a domain you control You can run through the same instructions with other reverse proxies such as Nginx , or Traefik . Scenario: * You want to share a file such as a VM image or a ISO over the Internet, with HTTPS, directly from your laptop. * You have one or more websites or APIs running on-premises or within your home-lab and want to expose them on the Internet. You can subscribe to inlets for personal or commercial use via Gumroad","title":"Pre-reqs"},{"location":"tutorial/caddy-http-tunnel/#setup-your-exit-node","text":"Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl : inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --pro Note the --url and TOKEN given to you in this step.","title":"Setup your exit node"},{"location":"tutorial/caddy-http-tunnel/#setup-your-dns-a-record","text":"Setup a DNS A record for the site you want to expose using the public IP of the cloud VM 178.128.40.109 = service.example.com","title":"Setup your DNS A record"},{"location":"tutorial/caddy-http-tunnel/#run-a-local-server-to-share-files","text":"Do not run this command in your home folder. mkdir -p /tmp/shared/ cd /tmp/shared/ echo \"Hello world\" > WELCOME.txt # If using Python 2.x python -m SimpleHTTPServer # Python 3.x python3 -m http.server This will listen on port 8000 by default.","title":"Run a local server to share files"},{"location":"tutorial/caddy-http-tunnel/#setup-caddy-1x","text":"Download the latest Caddy 1.x binary from the Releases page Pick your operating system, for instance Darwin for MacOS, or Linux. Download the binary, extract it and install it to /usr/local/bin : mkdir -p /tmp/caddy curl -sLSf https://github.com/caddyserver/caddy/releases/download/v1.0.4/caddy_v1.0.4_darwin_amd64.zip > caddy.tar.gz tar -xvf caddy.tar.gz --strip-components = 0 -C /tmp/caddy sudo cp /tmp/caddy/caddy /usr/local/bin/ Create a Caddyfile The Caddyfile configures which websites Caddy will expose, and which sites need a TLS certificate. Replace service.example.com with your own domain. Next, edit proxy / 127.0.0.1:8000 and change the port 8000 to the port of your local webserver, for instance 3000 or 8080 . For our example, keep it as 8000 . service.example.com proxy / 127 .0.0.1:8000 { transparent } Start the Caddy binary, it will listen on port 80 and 443. sudo ./caddy If you have more than one website, you can add them to the Caddyfile on new lines. You'll need to run caddy as sudo so that it can bind to ports 80, and 443 which require additional privileges.","title":"Setup Caddy 1.x"},{"location":"tutorial/caddy-http-tunnel/#start-the-inlets-pro-client-on-your-local-side","text":"Downloads the inlets PRO client: sudo inletsctl download Run the inlets-pro client, using the TOKEN and IP given to you from the previous step. The client will look for your license in $HOME/.inlets/LICENSE , but you can also use the --license/--license-file flag if you wish. export IP = \"\" # take this from the exit-server export TOKEN = \"\" # take this from the exit-server inlets-pro client \\ --url wss:// $IP :8123/connect \\ --ports 80 ,443 \\ --token $TOKEN \\ --upstream localhost Note that --upstream localhost will connect to Caddy running on your computer, if you are running Caddy on another machine, use its IP address here.","title":"Start the inlets-pro client on your local side"},{"location":"tutorial/caddy-http-tunnel/#check-it-all-worked","text":"You'll see that Caddy can now obtain a TLS certificate. Go ahead and visit: https://service.example.com Congratulations, you've now served a TLS certificate directly from your laptop. You can close caddy and open it again at a later date. Caddy will re-use the certificate it already obtained and it will be valid for 3 months. To renew, just keep Caddy running or open it again whenever you need it.","title":"Check it all worked"},{"location":"tutorial/caddy-http-tunnel/#setup-caddy-2x","text":"For Caddy 2.x, the Caddyfile format changes. Let's say you're running a Node.js service on port 3000, and want to expose it with TLS on the domain \"service.example.com\": git clone https://github.com/alexellis/expressjs-k8s/ cd expressjs-k8s npm install http_port=3000 npm start The local site will be served at http://127.0.0.1:3000 { acme_ca https://acme-staging-v02.api.letsencrypt.org/directory } service.example.com reverse_proxy 127.0.0.1:3000 { } Note the acme_ca being used will receive a staging certificate, remove it to obtain a production TLS certificate. Now download Caddy 2.x for your operating system. sudo ./caddy run \\ -config ./Caddyfile sudo - is required to bind to port 80 and 443, although you can potentially update your OS to allow binding to low ports without root access. You should now be able to access the Node.js website via the https://service.example.com URL. Caddy also supports multiple domains within the same file, so that you can expose multiple internal or private websites through the same tunnel. { email \"webmaster@example.com\" } blog.example.com { reverse_proxy 127.0.0.1:4000 } openfaas.example.com { reverse_proxy 127.0.0.1:8080 } If you have services running on other machines you can change 127.0.0.1:8080 to a different IP address such as that of your Raspberry Pi if you had something like OpenFaaS running there.","title":"Setup Caddy 2.x"},{"location":"tutorial/community/","text":"Community tutorials and guides \u00b6 Note: Any material not hosted on inlets.dev may be written by a third-party. If you have a tutorial or video to submit, feel free to send a Pull Request Case studies \u00b6 You can read testimonials on the main homepage Connecting my boat to the Internet with inlets by Mark Sharpley How Riskfuel is using Inlets to build machine learning models at scale by Addison van den Hoeven Ingress to ECS Anywhere, from anywhere, using Inlets by Nathan Peck Reliable local port-forwarding from Kubernetes for a Developer at UK Gov Videos \u00b6 Webinars: A tale of two networks - demos and use-cases for inlets tunnels (Mar 2021) by Alex Ellis and Johan Siebens Crossing network boundaries with Kubernetes and inlets (Mar 2021) by Alex Ellis and Johan Siebens Walk-through videos: inlets-operator - Get Ingress and Public IPs for private Kubernetes (Mar 2020) by Alex Ellis Inlets Operator - get a LoadBalancer from any Kubernetes cluster (Oct 2019) by Alex Ellis Hacking on the Inlets Operator for Equinix Metal (Jul 2021) by Alex Ellis and David McKay Tutorials \u00b6 A Tour of Inlets - A Tunnel Built for the Cloud (Aug 2021) by Zespre Schmidt Control Access to your on-prem services with Cloud IAP and inlets PRO (Dec 2020) by Johan Siebens Secure access using HashiCorp Boundary & inlets PRO Better Together (Oct 2020) by Johan Siebens Quake III Arena, k3s and a Raspberry Pi (Nov 2020) by Johan Siebens Argo CD for your private Raspberry Pi k3s cluster (Aug 2020) by Johan Siebens Get a TLS-enabled Docker registry in 5 minutes (Feb 2020) by Alex Ellis A bit of Istio before tea-time (May 2021) by Alex Ellis Get kubectl access to your private cluster from anywhere (Jan 2020) by Alex Ellis Exploring Kubernetes Operator Pattern (Jan 2021) by Ivan Velichko Official blog posts \u00b6 See inlets.dev/blog","title":"Community tutorials and guides"},{"location":"tutorial/community/#community-tutorials-and-guides","text":"Note: Any material not hosted on inlets.dev may be written by a third-party. If you have a tutorial or video to submit, feel free to send a Pull Request","title":"Community tutorials and guides"},{"location":"tutorial/community/#case-studies","text":"You can read testimonials on the main homepage Connecting my boat to the Internet with inlets by Mark Sharpley How Riskfuel is using Inlets to build machine learning models at scale by Addison van den Hoeven Ingress to ECS Anywhere, from anywhere, using Inlets by Nathan Peck Reliable local port-forwarding from Kubernetes for a Developer at UK Gov","title":"Case studies"},{"location":"tutorial/community/#videos","text":"Webinars: A tale of two networks - demos and use-cases for inlets tunnels (Mar 2021) by Alex Ellis and Johan Siebens Crossing network boundaries with Kubernetes and inlets (Mar 2021) by Alex Ellis and Johan Siebens Walk-through videos: inlets-operator - Get Ingress and Public IPs for private Kubernetes (Mar 2020) by Alex Ellis Inlets Operator - get a LoadBalancer from any Kubernetes cluster (Oct 2019) by Alex Ellis Hacking on the Inlets Operator for Equinix Metal (Jul 2021) by Alex Ellis and David McKay","title":"Videos"},{"location":"tutorial/community/#tutorials","text":"A Tour of Inlets - A Tunnel Built for the Cloud (Aug 2021) by Zespre Schmidt Control Access to your on-prem services with Cloud IAP and inlets PRO (Dec 2020) by Johan Siebens Secure access using HashiCorp Boundary & inlets PRO Better Together (Oct 2020) by Johan Siebens Quake III Arena, k3s and a Raspberry Pi (Nov 2020) by Johan Siebens Argo CD for your private Raspberry Pi k3s cluster (Aug 2020) by Johan Siebens Get a TLS-enabled Docker registry in 5 minutes (Feb 2020) by Alex Ellis A bit of Istio before tea-time (May 2021) by Alex Ellis Get kubectl access to your private cluster from anywhere (Jan 2020) by Alex Ellis Exploring Kubernetes Operator Pattern (Jan 2021) by Ivan Velichko","title":"Tutorials"},{"location":"tutorial/community/#official-blog-posts","text":"See inlets.dev/blog","title":"Official blog posts"},{"location":"tutorial/dual-tunnels/","text":"Setting up dual TCP and HTTPS tunnels \u00b6 In this tutorial we will set both a dual tunnel for exposing HTTP and TCP services from the same server. Whilst it's easier to automate two separate servers or cloud instances for your tunnels, you may want to reduce your costs. The use-case may be that you have a number of OpenFaaS functions running on your Raspberry Pi which serve traffic to users, but you also want to connect via SSH and VNC. Pre-reqs \u00b6 A Linux server, Windows and MacOS are also supported The inlets-pro binary at /usr/local/bin/ Access to a DNS control plane for a domain you control Create the HTTPS tunnel server first \u00b6 Create a HTTPS tunnel server using the manual tutorial or automated tutorial . Once it's running, check you can connect to it, and then log in with SSH. You'll find a systemd service named inlets-pro running the HTTPS tunnel with a specific authentication token and set of parameters. Now, generate a new systemd unit file for the TCP tunnel. I would suggest generating a new token for this tunnel. TOKEN = \" $( head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1 ) \" # Find the instance's public IPv4 address: PUBLIC_IP = \" $( curl -s https://checkip.amazonaws.com ) \" Let's imagine the public IP resolved to 46.101.128.5 which is part of the DigitalOcean range. inlets-pro tcp server \\ --token \" $TOKEN \" \\ --auto-tls-san $PUBLIC_IP \\ --generate = systemd > inlets-pro-tcp.service Example: [Unit] Description = inlets PRO TCP Server After = network.target [Service] Type = simple Restart = always RestartSec = 5 StartLimitInterval = 0 ExecStart = /usr/local/bin/inlets-pro tcp server --auto-tls --auto-tls-san=46.101.128.5 --control-addr=0.0.0.0 --token=\"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" --control-port=8124 --auto-tls-path=/tmp/inlets-pro-tcp [Install] WantedBy = multi-user.target We need to update the control-port for this inlets tunnel server via the --control-port flag. Use port 8124 since 8123 is already in use by the HTTP tunnel. Add --control-port 8124 to the ExecStart line. We need to add a new flag so that generated TLS certificates are placed in a unique directory, and don't clash. Add --auto-tls-path /tmp/inlets-pro-tcp/ to the same line. Next install the unit file with: sudo cp inlets-pro-tcp.service /etc/systemd/system/ sudo systemctl daemon-reload sudo systemctl enable inlets-pro-tcp.service sudo systemctl restart inlets-pro-tcp.service You'll now be able to check the logs for the server: sudo journalctl -u inlets-pro-tcp Finally you can connect your TCP client: inlets-pro tcp client \\ --token \"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" \\ --upstream 192 .168.0.15 \\ --ports 2222 ,5900 \\ --url wss://46.101.128.5:8124 Note that 5900 is the default port for VNC. Port 2222 was used for SSH as not to conflict with the version running on the tunnel server. You can now connect to the public IP of your server via SSH and VNC: For example: ssh -p 2222 pi@46.101.128.5 Wrapping up \u00b6 You now have a TCP and HTTPS tunnel server running on the same host. This was made possibly by changing the control-plane port and auto-TLS path for the second server, and having it start automatically through a separate systemd service. This technique may save you a few dollars per month, but it may not be worth your time compared to how quick and easy it is to create two separate servers with inletsctl create .","title":"Dual tunnels"},{"location":"tutorial/dual-tunnels/#setting-up-dual-tcp-and-https-tunnels","text":"In this tutorial we will set both a dual tunnel for exposing HTTP and TCP services from the same server. Whilst it's easier to automate two separate servers or cloud instances for your tunnels, you may want to reduce your costs. The use-case may be that you have a number of OpenFaaS functions running on your Raspberry Pi which serve traffic to users, but you also want to connect via SSH and VNC.","title":"Setting up dual TCP and HTTPS tunnels"},{"location":"tutorial/dual-tunnels/#pre-reqs","text":"A Linux server, Windows and MacOS are also supported The inlets-pro binary at /usr/local/bin/ Access to a DNS control plane for a domain you control","title":"Pre-reqs"},{"location":"tutorial/dual-tunnels/#create-the-https-tunnel-server-first","text":"Create a HTTPS tunnel server using the manual tutorial or automated tutorial . Once it's running, check you can connect to it, and then log in with SSH. You'll find a systemd service named inlets-pro running the HTTPS tunnel with a specific authentication token and set of parameters. Now, generate a new systemd unit file for the TCP tunnel. I would suggest generating a new token for this tunnel. TOKEN = \" $( head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1 ) \" # Find the instance's public IPv4 address: PUBLIC_IP = \" $( curl -s https://checkip.amazonaws.com ) \" Let's imagine the public IP resolved to 46.101.128.5 which is part of the DigitalOcean range. inlets-pro tcp server \\ --token \" $TOKEN \" \\ --auto-tls-san $PUBLIC_IP \\ --generate = systemd > inlets-pro-tcp.service Example: [Unit] Description = inlets PRO TCP Server After = network.target [Service] Type = simple Restart = always RestartSec = 5 StartLimitInterval = 0 ExecStart = /usr/local/bin/inlets-pro tcp server --auto-tls --auto-tls-san=46.101.128.5 --control-addr=0.0.0.0 --token=\"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" --control-port=8124 --auto-tls-path=/tmp/inlets-pro-tcp [Install] WantedBy = multi-user.target We need to update the control-port for this inlets tunnel server via the --control-port flag. Use port 8124 since 8123 is already in use by the HTTP tunnel. Add --control-port 8124 to the ExecStart line. We need to add a new flag so that generated TLS certificates are placed in a unique directory, and don't clash. Add --auto-tls-path /tmp/inlets-pro-tcp/ to the same line. Next install the unit file with: sudo cp inlets-pro-tcp.service /etc/systemd/system/ sudo systemctl daemon-reload sudo systemctl enable inlets-pro-tcp.service sudo systemctl restart inlets-pro-tcp.service You'll now be able to check the logs for the server: sudo journalctl -u inlets-pro-tcp Finally you can connect your TCP client: inlets-pro tcp client \\ --token \"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" \\ --upstream 192 .168.0.15 \\ --ports 2222 ,5900 \\ --url wss://46.101.128.5:8124 Note that 5900 is the default port for VNC. Port 2222 was used for SSH as not to conflict with the version running on the tunnel server. You can now connect to the public IP of your server via SSH and VNC: For example: ssh -p 2222 pi@46.101.128.5","title":"Create the HTTPS tunnel server first"},{"location":"tutorial/dual-tunnels/#wrapping-up","text":"You now have a TCP and HTTPS tunnel server running on the same host. This was made possibly by changing the control-plane port and auto-TLS path for the second server, and having it start automatically through a separate systemd service. This technique may save you a few dollars per month, but it may not be worth your time compared to how quick and easy it is to create two separate servers with inletsctl create .","title":"Wrapping up"},{"location":"tutorial/kubernetes-api-server/","text":"Tutorial: Expose a local Kubernetes API Server \u00b6 In this tutorial, we'll show you how to expose a local Kubernetes API Server on the Internet, so that you can access it from anywhere, just like with a managed cloud provider. Pre-reqs \u00b6 A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already Kubernetes running locally with kubeadm, K3s, K3d, Minikube, KinD, Docker Desktop, etc The Kubernetes cluster \u00b6 By default every Kubernetes cluster has TLS enabled to encrypt any HTTP REST messages that go over its control-plane. The TLS certificate has to be bound to a certain name, sometimes called a TLS SAN. The certificate is usually only valid for \"kubernetes.default.svc\", and can only be accessed from within the cluster. Kubernetes on tour - get access to your cluster from anywhere, without having to resort to complex tooling like VPNs. When a managed cloud provider provisions you a cluster, they'll add additional names into the certificate like \"customer1.lke.eu.linode.com\" which is then added to your generated kubeconfig file that you download in the dashboard. We have five steps run through to expose the API server: 1) Create a Kubernetes cluster 2) Create a VM on the public cloud with an inlets TCP server running on it 3) Create a DNS entry for the public VM's IP address 4) Configure a TLS SAN, if possible with a new domain name 5) Set up an inlets client as a Pod to forward traffic to the Kubernetes API Server Once we have all this in place, we can take our existing kubeconfig file and edit the URL, so that instead of pointing at our LAN IP or localhost, it points to the domain mapped to the public VM. Create a cluster \u00b6 You can create a cluster on any machine by using KinD: arkade get kind kind create cluster If you have a Raspberry Pi or a Linux Server, you can install K3s using k3sup : arkade get k3sup k3sup install --ip 192 .168.1.101 --user pi In either case, you'll get back a kubeconfig file. Here's a snippet of what I got back from running k3sup install : apiVersion : v1 clusters : - cluster : server : https://192.168.1.101:6443 The server field will need to be changed to the new public address later on. Create a VM on the public cloud with an inlets TCP server running on it \u00b6 Just like when Linode Kubernetes Engine provisions us a domain like \"customer1.lke.eu.linode.com\" , we'll need our own subdomain too, so that the certificate can be issued for it. In order to create the DNS record, we a public IP which we will get by creating a tunnel server on our preferred cloud and in a region that's close to us . arkade get inletsctl export ACCESS_TOKEN = \"\" # Retreive this from your cloud dashboard inletsctl create \\ --provider linode \\ --tcp \\ --access-token $ACCESS_TOKEN \\ --region eu-west Save the connection info from inletsctl into a text file for later. # Give a single value or comma-separated export PORTS=\"8000\" # Where to route traffic from the inlets server export UPSTREAM=\"localhost\" inlets-pro tcp client --url \"wss://139.160.201.143:8123\" \\ --token \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" \\ --upstream $UPSTREAM \\ --ports $PORTS Create a DNS subdomain for the IP address you were given: k3s.example.com => 139.160.201.143 Check that you can resolve the IP with a ping ping -c 1 k3s.example.com Now check the status of the inlets server: export TOKEN = \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" inlets-pro status --url \"wss://139.160.201.143:8123\" \\ --token \" $TOKEN \" Output: inlets server status. Version: 0 .9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 Server info: Hostname: localhost Process uptime: 5 seconds ago Mode: tcp Version: 0 .9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 No clients connected. We can now move onto the next step. Configure a TLS SAN, if possible with a new domain name \u00b6 With k3s, it's trivial to add additional TLS SAN names for the Kubernetes API Server. If you run the k3sup install command again, it'll update your configuration: k3sup install \\ --ip 192 .168.1.101 \\ --user pi \\ --tls-san k3s.example.com You'll now have the custom domain along with the default kubernetes.default.svc as valid names in the generated certificate. If you're not running on k3s, or use a service where you cannot change the TLS SAN, then we'll show you what to do in the next step. Update your kubeconfig file with the new endpoint \u00b6 We need to update our kubeconfig file to point at the custom domain instead of at whatever loopback or LAN address it currently does. For K3s users, change the server URL: apiVersion : v1 clusters : - cluster : server : https://192.168.1.101:6443 To: apiVersion : v1 clusters : - cluster : server : https://k3s.example.com:443 For any user where you cannot regenerate the TLS certificate for the API Server, you can specify the server name in the config file: apiVersion : v1 clusters : - cluster : server : https://k3s.example.com:443 tls-server-name : k3s.example.com For more details see: Support TLS Server Name overrides in kubeconfig file #88769 Save the changes to your kubeconfig file. Connect the tunnel \u00b6 The tunnel acts like a router, it takes any TCP packets sent to port 6443 (k3s) or 443 (Kubernetes) and forwards them down the tunnel to the inlets client. The inlets client then looks at its own \"--upstream\" value to decide where to finally send the data. Save inlets-k8s-api.yaml : export LICENSE = \" $( cat $HOME /.inlets/LICENSE ) \" export TOKEN = \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" # populate with the token from inletsctl export SERVER_IP = \"139.160.201.143\" # populate with the server IP, not the domain cat > inlets-k8s-api.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: inlets-client spec: replicas: 1 selector: matchLabels: app: inlets-client template: metadata: labels: app: inlets-client spec: containers: - name: inlets-client image: ghcr.io/inlets/inlets-pro:0.9.3 imagePullPolicy: IfNotPresent command: [\"inlets-pro\"] args: - \"tcp\" - \"client\" - \"--url=wss://$SERVER_IP:8123\" - \"--upstream=kubernetes.default.svc\" - \"--port=443\" - \"--port=6443\" - \"--token=$TOKEN\" - \"--license=$LICENSE\" --- EOF You'll see the tunnel client up and running and ready to receive requests: kubectl logs deploy/inlets-client 2022/06/24 09:51:18 Licensed to: Alex <contact@openfaas.com>, expires: 128 day(s) 2022/06/24 09:51:18 Upstream server: kubernetes.default.svc, for ports: 443, 6443 time=\"2022/06/24 09:51:18\" level=info msg=\"Connecting to proxy\" url=\"wss://139.160.201.143:8123/connect\" inlets-pro TCP client. Copyright OpenFaaS Ltd 2021 time=\"2022/06/24 09:51:18\" level=info msg=\"Connection established\" client_id=5309466072564c1c90ce0a0bcaa22b74 Check the tunnel server's status to confirm the connection: export TOKEN = \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" inlets-pro status --url \"wss://139.160.201.143:8123\" \\ --token \" $TOKEN \" inlets server status. Version: 0 .9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 Server info: Hostname: localhost Process uptime: 15 minutes ago Mode: tcp Version: 0 .9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 Connected clients: Client ID Remote Address Connected Upstreams 5309466072564c1c90ce0a0bcaa22b74 192 .168.1.101:16368 43 seconds kubernetes.default.svc:443, kubernetes.default.svc:6443 Finally prove that it's working with the new, public address: $ kubectl cluster-info Kubernetes control plane is running at https://k3s.example.com:443 CoreDNS is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' . Wrapping up \u00b6 In a relatively short period of time, with a custom domain, and a small VM, we set up a tunnel server to route traffic from the public Internet to a K3s server on an internal network. This gives you a similar experience to a managed public cloud Kubernetes engine, but running on your own infrastructure, or perhaps within a restrictive VPC. You may also like: Learn how to manage apps across multiple Kubernetes clusters by Johan Siebens If you'd like to talk to us about this tutorial, feel free to reach out for a meeting: Set up a meeting","title":"Tutorial: Expose a local Kubernetes API Server"},{"location":"tutorial/kubernetes-api-server/#tutorial-expose-a-local-kubernetes-api-server","text":"In this tutorial, we'll show you how to expose a local Kubernetes API Server on the Internet, so that you can access it from anywhere, just like with a managed cloud provider.","title":"Tutorial: Expose a local Kubernetes API Server"},{"location":"tutorial/kubernetes-api-server/#pre-reqs","text":"A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already Kubernetes running locally with kubeadm, K3s, K3d, Minikube, KinD, Docker Desktop, etc","title":"Pre-reqs"},{"location":"tutorial/kubernetes-api-server/#the-kubernetes-cluster","text":"By default every Kubernetes cluster has TLS enabled to encrypt any HTTP REST messages that go over its control-plane. The TLS certificate has to be bound to a certain name, sometimes called a TLS SAN. The certificate is usually only valid for \"kubernetes.default.svc\", and can only be accessed from within the cluster. Kubernetes on tour - get access to your cluster from anywhere, without having to resort to complex tooling like VPNs. When a managed cloud provider provisions you a cluster, they'll add additional names into the certificate like \"customer1.lke.eu.linode.com\" which is then added to your generated kubeconfig file that you download in the dashboard. We have five steps run through to expose the API server: 1) Create a Kubernetes cluster 2) Create a VM on the public cloud with an inlets TCP server running on it 3) Create a DNS entry for the public VM's IP address 4) Configure a TLS SAN, if possible with a new domain name 5) Set up an inlets client as a Pod to forward traffic to the Kubernetes API Server Once we have all this in place, we can take our existing kubeconfig file and edit the URL, so that instead of pointing at our LAN IP or localhost, it points to the domain mapped to the public VM.","title":"The Kubernetes cluster"},{"location":"tutorial/kubernetes-api-server/#create-a-cluster","text":"You can create a cluster on any machine by using KinD: arkade get kind kind create cluster If you have a Raspberry Pi or a Linux Server, you can install K3s using k3sup : arkade get k3sup k3sup install --ip 192 .168.1.101 --user pi In either case, you'll get back a kubeconfig file. Here's a snippet of what I got back from running k3sup install : apiVersion : v1 clusters : - cluster : server : https://192.168.1.101:6443 The server field will need to be changed to the new public address later on.","title":"Create a cluster"},{"location":"tutorial/kubernetes-api-server/#create-a-vm-on-the-public-cloud-with-an-inlets-tcp-server-running-on-it","text":"Just like when Linode Kubernetes Engine provisions us a domain like \"customer1.lke.eu.linode.com\" , we'll need our own subdomain too, so that the certificate can be issued for it. In order to create the DNS record, we a public IP which we will get by creating a tunnel server on our preferred cloud and in a region that's close to us . arkade get inletsctl export ACCESS_TOKEN = \"\" # Retreive this from your cloud dashboard inletsctl create \\ --provider linode \\ --tcp \\ --access-token $ACCESS_TOKEN \\ --region eu-west Save the connection info from inletsctl into a text file for later. # Give a single value or comma-separated export PORTS=\"8000\" # Where to route traffic from the inlets server export UPSTREAM=\"localhost\" inlets-pro tcp client --url \"wss://139.160.201.143:8123\" \\ --token \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" \\ --upstream $UPSTREAM \\ --ports $PORTS Create a DNS subdomain for the IP address you were given: k3s.example.com => 139.160.201.143 Check that you can resolve the IP with a ping ping -c 1 k3s.example.com Now check the status of the inlets server: export TOKEN = \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" inlets-pro status --url \"wss://139.160.201.143:8123\" \\ --token \" $TOKEN \" Output: inlets server status. Version: 0 .9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 Server info: Hostname: localhost Process uptime: 5 seconds ago Mode: tcp Version: 0 .9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 No clients connected. We can now move onto the next step.","title":"Create a VM on the public cloud with an inlets TCP server running on it"},{"location":"tutorial/kubernetes-api-server/#configure-a-tls-san-if-possible-with-a-new-domain-name","text":"With k3s, it's trivial to add additional TLS SAN names for the Kubernetes API Server. If you run the k3sup install command again, it'll update your configuration: k3sup install \\ --ip 192 .168.1.101 \\ --user pi \\ --tls-san k3s.example.com You'll now have the custom domain along with the default kubernetes.default.svc as valid names in the generated certificate. If you're not running on k3s, or use a service where you cannot change the TLS SAN, then we'll show you what to do in the next step.","title":"Configure a TLS SAN, if possible with a new domain name"},{"location":"tutorial/kubernetes-api-server/#update-your-kubeconfig-file-with-the-new-endpoint","text":"We need to update our kubeconfig file to point at the custom domain instead of at whatever loopback or LAN address it currently does. For K3s users, change the server URL: apiVersion : v1 clusters : - cluster : server : https://192.168.1.101:6443 To: apiVersion : v1 clusters : - cluster : server : https://k3s.example.com:443 For any user where you cannot regenerate the TLS certificate for the API Server, you can specify the server name in the config file: apiVersion : v1 clusters : - cluster : server : https://k3s.example.com:443 tls-server-name : k3s.example.com For more details see: Support TLS Server Name overrides in kubeconfig file #88769 Save the changes to your kubeconfig file.","title":"Update your kubeconfig file with the new endpoint"},{"location":"tutorial/kubernetes-api-server/#connect-the-tunnel","text":"The tunnel acts like a router, it takes any TCP packets sent to port 6443 (k3s) or 443 (Kubernetes) and forwards them down the tunnel to the inlets client. The inlets client then looks at its own \"--upstream\" value to decide where to finally send the data. Save inlets-k8s-api.yaml : export LICENSE = \" $( cat $HOME /.inlets/LICENSE ) \" export TOKEN = \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" # populate with the token from inletsctl export SERVER_IP = \"139.160.201.143\" # populate with the server IP, not the domain cat > inlets-k8s-api.yaml <<EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: inlets-client spec: replicas: 1 selector: matchLabels: app: inlets-client template: metadata: labels: app: inlets-client spec: containers: - name: inlets-client image: ghcr.io/inlets/inlets-pro:0.9.3 imagePullPolicy: IfNotPresent command: [\"inlets-pro\"] args: - \"tcp\" - \"client\" - \"--url=wss://$SERVER_IP:8123\" - \"--upstream=kubernetes.default.svc\" - \"--port=443\" - \"--port=6443\" - \"--token=$TOKEN\" - \"--license=$LICENSE\" --- EOF You'll see the tunnel client up and running and ready to receive requests: kubectl logs deploy/inlets-client 2022/06/24 09:51:18 Licensed to: Alex <contact@openfaas.com>, expires: 128 day(s) 2022/06/24 09:51:18 Upstream server: kubernetes.default.svc, for ports: 443, 6443 time=\"2022/06/24 09:51:18\" level=info msg=\"Connecting to proxy\" url=\"wss://139.160.201.143:8123/connect\" inlets-pro TCP client. Copyright OpenFaaS Ltd 2021 time=\"2022/06/24 09:51:18\" level=info msg=\"Connection established\" client_id=5309466072564c1c90ce0a0bcaa22b74 Check the tunnel server's status to confirm the connection: export TOKEN = \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" inlets-pro status --url \"wss://139.160.201.143:8123\" \\ --token \" $TOKEN \" inlets server status. Version: 0 .9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 Server info: Hostname: localhost Process uptime: 15 minutes ago Mode: tcp Version: 0 .9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514 Connected clients: Client ID Remote Address Connected Upstreams 5309466072564c1c90ce0a0bcaa22b74 192 .168.1.101:16368 43 seconds kubernetes.default.svc:443, kubernetes.default.svc:6443 Finally prove that it's working with the new, public address: $ kubectl cluster-info Kubernetes control plane is running at https://k3s.example.com:443 CoreDNS is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump' .","title":"Connect the tunnel"},{"location":"tutorial/kubernetes-api-server/#wrapping-up","text":"In a relatively short period of time, with a custom domain, and a small VM, we set up a tunnel server to route traffic from the public Internet to a K3s server on an internal network. This gives you a similar experience to a managed public cloud Kubernetes engine, but running on your own infrastructure, or perhaps within a restrictive VPC. You may also like: Learn how to manage apps across multiple Kubernetes clusters by Johan Siebens If you'd like to talk to us about this tutorial, feel free to reach out for a meeting: Set up a meeting","title":"Wrapping up"},{"location":"tutorial/kubernetes-ingress/","text":"Tutorial: Expose a local IngressController with the inlets-operator \u00b6 In this quick-start we will configure the inlets-operator to use inlets-pro (a TCP proxy) to expose NginxIngress so that it can receive HTTPS certificates via LetsEncrypt and cert-manager . You can subscribe to inlets for personal or commercial use via Gumroad If you don't have a license for inlets PRO, then your IngressController will only be able to serve plaintext HTTP over port 80 and you won't be able to obtain a TLS certificate. Pre-reqs \u00b6 A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already KinD - the \"darling\" of the Kubernetes community is Kubernetes IN Docker, a small one-shot cluster that can run inside a Docker container arkade - arkade is an app installer that takes a helm chart and bundles it behind a simple CLI Install arkade \u00b6 You can use arkade or helm to install the various applications we are going to add to the cluster below. arkade provides an apps ecosystem that makes things much quicker. MacOS and Linux users: curl -sSLf https://dl.get-arkade.dev/ | sudo sh Windows users should install Git Bash and run the above without `sudo. Create a Kubernetes cluster with KinD \u00b6 We're going to use KinD , which runs inside a container with Docker for Mac or the Docker daemon. MacOS cannot actually run containers or Kubernetes itself, so projects like Docker for Mac create a small Linux VM and hide it away. You can use an alternative to KinD if you have a preferred tool. Get a KinD binary release and kubectl (the Kubernetes CLI): arkade get kind --version v0.9.0 arkade get kubectl --version v1.19.3 Now create a cluster: $ kind create cluster The initial creation could take a few minutes, but subsequent clusters creations are much faster. Creating cluster \"kind\" ... \u2713 Ensuring node image (kindest/node:v1.19.0) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! \ud83d\udc4b We can check that our single node is ready now: kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-control-plane Ready master 35s v1.18.0 172 .17.0.2 <none> Ubuntu 19 .10 5 .3.0-26-generic containerd://1.3.2 The above shows one node Ready, so we are ready to move on. Install the inlets-operator \u00b6 Save an access token for your cloud provider as $HOME/access-token , in this example we're using DigitalOcean. Make sure you set LICENSE_FILE with the path to your license file, normally saved at: $HOME/.inlets/LICENSE . export ACCESS_TOKEN = $HOME /access-token export LICENSE_FILE = \" $HOME /LICENSE.txt\" arkade install inlets-operator \\ --provider digitalocean \\ --region lon1 \\ --token-file $ACCESS_TOKEN \\ --license-file \" $LICENSE_FILE \" You can run arkade install inlets-operator --help to see a list of other cloud providers. Set the --region flag as required, it's best to have low latency between your current location and where the exit-servers will be provisioned. Use your license in --license , or omit this flag if you just want to serve port 80 from your IngressController without any TLS Install nginx-ingress \u00b6 This installs nginx-ingress using its Helm chart: arkade install nginx-ingress Install cert-manager \u00b6 Install cert-manager , which can obtain TLS certificates through NginxIngress. arkade install cert-manager A quick review \u00b6 Here's what we have so far: nginx-ingress An IngressController, Traefik or Caddy are also valid options. It comes with a Service of type LoadBalancer that will get a public address via the tunnel inlets-operator configured to use inlets-pro Provides us with a public VirtualIP for the IngressController service. cert-manager Provides TLS certificates through the HTTP01 or DNS01 challenges from LetsEncrypt Deploy an application and get a TLS certificate \u00b6 This is the final step that shows everything working end to end. TLS certificates require a domain name and DNS A or CNAME entry, so let's set that up Find the External-IP: kubectl get svc Now create a DNS A record in your admin panel, so for example: expressjs.example.com . Now when you install a Kubernetes application with an Ingress definition, NginxIngress and cert-manager will work together to provide a TLS certificate. Create a staging issuer for cert-manager staging-issuer.yaml and make sure you edit the email value. export EMAIL = \"you@example.com\" cat > issuer-staging.yaml <<EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-staging namespace: default spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: $EMAIL privateKeySecretRef: name: letsencrypt-staging solvers: - selector: {} http01: ingress: class: nginx EOF Apply the file with kubectl apply -f staging-issuer.yaml While the Let's Encrypt production server has strict limits on the API, the staging server is more forgiving, and should be used while you are testing a deployment. Edit email , then run: kubectl apply -f issuer.yaml . Let's use helm3 to install Alex's example Node.js API available on GitHub Create a custom.yaml file with the following: ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx cert-manager.io/issuer : \"letsencrypt-staging\" hosts : - host : expressjs.inlets.dev paths : [ \"/\" ] tls : - secretName : expressjs-tls hosts : - expressjs.inlets.dev Replace the string expressjs.inlets.dev with your own sub-domain created earlier i.e. expressjs.example.com . If you are intending to deploy the chart to a Raspberry Pi, also update the image to an ARM version in the custom.yaml file: image : alexellis2/service:0.3.6-armhf You can download around a dozen other CLI tools using arkade including helm. Use arkade to download helm and put it in your PATH : arkade get helm # Put arkade in your path: export PATH = $PATH : $HOME /.arkade/bin/helm3/ # Or alternatively install to /usr/local/bin sudo cp $HOME /.arkade/bin/helm3/helm /usr/local/bin/ Now install the chart using helm: helm repo add expressjs-k8s https://alexellis.github.io/expressjs-k8s/ # Then they run an update helm repo update # And finally they install helm upgrade --install express expressjs-k8s/expressjs-k8s \\ --values custom.yaml Test it out \u00b6 Now check the certificate has been created and visit the webpage in a browser: kubectl get certificate NAME READY SECRET AGE expressjs-tls True expressjs-tls 49s Open the webpage i.e. https://api.example.com . Since this is a staging certificate, you will get a warning from your browser. You can accept the certificate in order to test your site. Getting a Production Certificate \u00b6 Create a production certificate issuer issuer-prod.yaml , similar to the staging issuer you produced earlier. Be sure to change the email address to your email. export EMAIL = \"you@example.com\" cat > issuer-prod.yaml <<EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: default spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $EMAIL privateKeySecretRef: name: letsencrypt-prod solvers: - selector: {} http01: ingress: class: nginx EOF Then run kubectl apply -f issuer-prod.yaml Now you must update your expressjs deployment to use the new certificate issuer. Create a new helm3 overrides file custom-prod.yaml : cat > custom-prod.yaml <<EOF ingress: enabled: true annotations: kubernetes.io/ingress.class: nginx cert-manager.io/issuer: \"letsencrypt-prod\" hosts: - host: expressjs.inlets.dev paths: [\"/\"] tls: - secretName: expressjs-tls hosts: - expressjs.inlets.dev EOF Be sure to change the above domain name to your domain name for the sample server. You can update your deployment using the helm command below: helm upgrade express expressjs-k8s/expressjs-k8s --values custom-prod.yaml Here's my example on my own domain: You can view the certificate the certificate that's being served directly from your local cluster and see that it's valid: Install a real-world application \u00b6 Using arkade you can now install OpenFaaS or a Docker Registry with a couple of commands, and since you have Nginx and cert-manager in place, this will only take a few moments. OpenFaaS with TLS \u00b6 OpenFaaS is a platform for Kubernetes that provides FaaS functionality and microservices. The motto of the project is Serverless Functions Made Simple and you can deploy it along with TLS in just a couple of commands: export DOMAIN = gateway.example.com arkade install openfaas arkade install openfaas-ingress \\ --email webmaster@ $DOMAIN \\ --domain $DOMAIN That's it, you'll now be able to access your gateway at https://$DOMAIN/ For more, see the OpenFaaS workshop Docker Registry with TLS \u00b6 A self-hosted Docker Registry with TLS and private authentication can be hard to set up, but we can now do that with two commands. export DOMAIN = registry.example.com arkade install docker-registry arkade install docker-registry-ingress \\ --email webmaster@ $DOMAIN \\ --domain $DOMAIN Now try your registry: docker login $DOMAIN docker pull alpine:3.11 docker tag alpine:3.11 $DOMAIN /alpine:3.11 docker push $DOMAIN /alpine:3.11 You can even combine the new private registry with OpenFaaS if you like, checkout the docs for more . Wrapping up \u00b6 Through the use of inlets-pro we have an encrypted control-plane for the websocket tunnel, and encryption for the traffic going to our Express.js app using a TLS certificate from LetsEncrypt. You can now get a green lock and a valid TLS certificate for your local cluster, which also means that this will work with bare-metal Kubernetes, on-premises and with your Raspberry Pi cluster. Note if you're just looking for something to use in development, without TLS or encryption, you can install the inlets-operator without the --license flag and port 80 will be exposed for you instead. You can still use NginxIngress, but you won't get a certificate and it won't be encrypted e2e.","title":"Tutorial: Expose a local IngressController with the inlets-operator"},{"location":"tutorial/kubernetes-ingress/#tutorial-expose-a-local-ingresscontroller-with-the-inlets-operator","text":"In this quick-start we will configure the inlets-operator to use inlets-pro (a TCP proxy) to expose NginxIngress so that it can receive HTTPS certificates via LetsEncrypt and cert-manager . You can subscribe to inlets for personal or commercial use via Gumroad If you don't have a license for inlets PRO, then your IngressController will only be able to serve plaintext HTTP over port 80 and you won't be able to obtain a TLS certificate.","title":"Tutorial: Expose a local IngressController with the inlets-operator"},{"location":"tutorial/kubernetes-ingress/#pre-reqs","text":"A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already KinD - the \"darling\" of the Kubernetes community is Kubernetes IN Docker, a small one-shot cluster that can run inside a Docker container arkade - arkade is an app installer that takes a helm chart and bundles it behind a simple CLI","title":"Pre-reqs"},{"location":"tutorial/kubernetes-ingress/#install-arkade","text":"You can use arkade or helm to install the various applications we are going to add to the cluster below. arkade provides an apps ecosystem that makes things much quicker. MacOS and Linux users: curl -sSLf https://dl.get-arkade.dev/ | sudo sh Windows users should install Git Bash and run the above without `sudo.","title":"Install arkade"},{"location":"tutorial/kubernetes-ingress/#create-a-kubernetes-cluster-with-kind","text":"We're going to use KinD , which runs inside a container with Docker for Mac or the Docker daemon. MacOS cannot actually run containers or Kubernetes itself, so projects like Docker for Mac create a small Linux VM and hide it away. You can use an alternative to KinD if you have a preferred tool. Get a KinD binary release and kubectl (the Kubernetes CLI): arkade get kind --version v0.9.0 arkade get kubectl --version v1.19.3 Now create a cluster: $ kind create cluster The initial creation could take a few minutes, but subsequent clusters creations are much faster. Creating cluster \"kind\" ... \u2713 Ensuring node image (kindest/node:v1.19.0) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! \ud83d\udc4b We can check that our single node is ready now: kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-control-plane Ready master 35s v1.18.0 172 .17.0.2 <none> Ubuntu 19 .10 5 .3.0-26-generic containerd://1.3.2 The above shows one node Ready, so we are ready to move on.","title":"Create a Kubernetes cluster with KinD"},{"location":"tutorial/kubernetes-ingress/#install-the-inlets-operator","text":"Save an access token for your cloud provider as $HOME/access-token , in this example we're using DigitalOcean. Make sure you set LICENSE_FILE with the path to your license file, normally saved at: $HOME/.inlets/LICENSE . export ACCESS_TOKEN = $HOME /access-token export LICENSE_FILE = \" $HOME /LICENSE.txt\" arkade install inlets-operator \\ --provider digitalocean \\ --region lon1 \\ --token-file $ACCESS_TOKEN \\ --license-file \" $LICENSE_FILE \" You can run arkade install inlets-operator --help to see a list of other cloud providers. Set the --region flag as required, it's best to have low latency between your current location and where the exit-servers will be provisioned. Use your license in --license , or omit this flag if you just want to serve port 80 from your IngressController without any TLS","title":"Install the inlets-operator"},{"location":"tutorial/kubernetes-ingress/#install-nginx-ingress","text":"This installs nginx-ingress using its Helm chart: arkade install nginx-ingress","title":"Install nginx-ingress"},{"location":"tutorial/kubernetes-ingress/#install-cert-manager","text":"Install cert-manager , which can obtain TLS certificates through NginxIngress. arkade install cert-manager","title":"Install cert-manager"},{"location":"tutorial/kubernetes-ingress/#a-quick-review","text":"Here's what we have so far: nginx-ingress An IngressController, Traefik or Caddy are also valid options. It comes with a Service of type LoadBalancer that will get a public address via the tunnel inlets-operator configured to use inlets-pro Provides us with a public VirtualIP for the IngressController service. cert-manager Provides TLS certificates through the HTTP01 or DNS01 challenges from LetsEncrypt","title":"A quick review"},{"location":"tutorial/kubernetes-ingress/#deploy-an-application-and-get-a-tls-certificate","text":"This is the final step that shows everything working end to end. TLS certificates require a domain name and DNS A or CNAME entry, so let's set that up Find the External-IP: kubectl get svc Now create a DNS A record in your admin panel, so for example: expressjs.example.com . Now when you install a Kubernetes application with an Ingress definition, NginxIngress and cert-manager will work together to provide a TLS certificate. Create a staging issuer for cert-manager staging-issuer.yaml and make sure you edit the email value. export EMAIL = \"you@example.com\" cat > issuer-staging.yaml <<EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-staging namespace: default spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: $EMAIL privateKeySecretRef: name: letsencrypt-staging solvers: - selector: {} http01: ingress: class: nginx EOF Apply the file with kubectl apply -f staging-issuer.yaml While the Let's Encrypt production server has strict limits on the API, the staging server is more forgiving, and should be used while you are testing a deployment. Edit email , then run: kubectl apply -f issuer.yaml . Let's use helm3 to install Alex's example Node.js API available on GitHub Create a custom.yaml file with the following: ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx cert-manager.io/issuer : \"letsencrypt-staging\" hosts : - host : expressjs.inlets.dev paths : [ \"/\" ] tls : - secretName : expressjs-tls hosts : - expressjs.inlets.dev Replace the string expressjs.inlets.dev with your own sub-domain created earlier i.e. expressjs.example.com . If you are intending to deploy the chart to a Raspberry Pi, also update the image to an ARM version in the custom.yaml file: image : alexellis2/service:0.3.6-armhf You can download around a dozen other CLI tools using arkade including helm. Use arkade to download helm and put it in your PATH : arkade get helm # Put arkade in your path: export PATH = $PATH : $HOME /.arkade/bin/helm3/ # Or alternatively install to /usr/local/bin sudo cp $HOME /.arkade/bin/helm3/helm /usr/local/bin/ Now install the chart using helm: helm repo add expressjs-k8s https://alexellis.github.io/expressjs-k8s/ # Then they run an update helm repo update # And finally they install helm upgrade --install express expressjs-k8s/expressjs-k8s \\ --values custom.yaml","title":"Deploy an application and get a TLS certificate"},{"location":"tutorial/kubernetes-ingress/#test-it-out","text":"Now check the certificate has been created and visit the webpage in a browser: kubectl get certificate NAME READY SECRET AGE expressjs-tls True expressjs-tls 49s Open the webpage i.e. https://api.example.com . Since this is a staging certificate, you will get a warning from your browser. You can accept the certificate in order to test your site.","title":"Test it out"},{"location":"tutorial/kubernetes-ingress/#getting-a-production-certificate","text":"Create a production certificate issuer issuer-prod.yaml , similar to the staging issuer you produced earlier. Be sure to change the email address to your email. export EMAIL = \"you@example.com\" cat > issuer-prod.yaml <<EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod namespace: default spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $EMAIL privateKeySecretRef: name: letsencrypt-prod solvers: - selector: {} http01: ingress: class: nginx EOF Then run kubectl apply -f issuer-prod.yaml Now you must update your expressjs deployment to use the new certificate issuer. Create a new helm3 overrides file custom-prod.yaml : cat > custom-prod.yaml <<EOF ingress: enabled: true annotations: kubernetes.io/ingress.class: nginx cert-manager.io/issuer: \"letsencrypt-prod\" hosts: - host: expressjs.inlets.dev paths: [\"/\"] tls: - secretName: expressjs-tls hosts: - expressjs.inlets.dev EOF Be sure to change the above domain name to your domain name for the sample server. You can update your deployment using the helm command below: helm upgrade express expressjs-k8s/expressjs-k8s --values custom-prod.yaml Here's my example on my own domain: You can view the certificate the certificate that's being served directly from your local cluster and see that it's valid:","title":"Getting a Production Certificate"},{"location":"tutorial/kubernetes-ingress/#install-a-real-world-application","text":"Using arkade you can now install OpenFaaS or a Docker Registry with a couple of commands, and since you have Nginx and cert-manager in place, this will only take a few moments.","title":"Install a real-world application"},{"location":"tutorial/kubernetes-ingress/#openfaas-with-tls","text":"OpenFaaS is a platform for Kubernetes that provides FaaS functionality and microservices. The motto of the project is Serverless Functions Made Simple and you can deploy it along with TLS in just a couple of commands: export DOMAIN = gateway.example.com arkade install openfaas arkade install openfaas-ingress \\ --email webmaster@ $DOMAIN \\ --domain $DOMAIN That's it, you'll now be able to access your gateway at https://$DOMAIN/ For more, see the OpenFaaS workshop","title":"OpenFaaS with TLS"},{"location":"tutorial/kubernetes-ingress/#docker-registry-with-tls","text":"A self-hosted Docker Registry with TLS and private authentication can be hard to set up, but we can now do that with two commands. export DOMAIN = registry.example.com arkade install docker-registry arkade install docker-registry-ingress \\ --email webmaster@ $DOMAIN \\ --domain $DOMAIN Now try your registry: docker login $DOMAIN docker pull alpine:3.11 docker tag alpine:3.11 $DOMAIN /alpine:3.11 docker push $DOMAIN /alpine:3.11 You can even combine the new private registry with OpenFaaS if you like, checkout the docs for more .","title":"Docker Registry with TLS"},{"location":"tutorial/kubernetes-ingress/#wrapping-up","text":"Through the use of inlets-pro we have an encrypted control-plane for the websocket tunnel, and encryption for the traffic going to our Express.js app using a TLS certificate from LetsEncrypt. You can now get a green lock and a valid TLS certificate for your local cluster, which also means that this will work with bare-metal Kubernetes, on-premises and with your Raspberry Pi cluster. Note if you're just looking for something to use in development, without TLS or encryption, you can install the inlets-operator without the --license flag and port 80 will be exposed for you instead. You can still use NginxIngress, but you won't get a certificate and it won't be encrypted e2e.","title":"Wrapping up"},{"location":"tutorial/manual-http-server/","text":"Setting up a HTTP tunnel server manually \u00b6 In this tutorial we will set up an inlets HTTP tunnel server to serve a local website over HTTPS using Let's Encrypt. The steps will be manual, but usually, we would use a provisioning tool like inletsctl to automate everything for us. This may be useful for understanding how the server binary works, and how to use it on existing servers that you may have. Or perhaps you want to run inlets across an internal or private network. Pre-reqs \u00b6 A Linux server, Windows and MacOS are also supported The inlets-pro binary at /usr/local/bin/ Access to a DNS control plane for a domain you control Run the server \u00b6 For this example, your tunnel server should be accessible from the Internet. The tunnel client will connect to it and then expose one or more local websites so that you can access them remotely. Create a DNS A record for the subdomain or subdomains you want to use, and have each of them point to the public IP address of the server you have provisioned. These short have a short TTL such as 60s to avoid waiting too long for DNS to propagate throughout the Internet. You can increase this value to a higher number later. First generate an authentication token that the client will use to log in: TOKEN = \" $( head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1 ) \" We'll use the built-in support for Let's Encrypt to get a valid HTTPS certificate for any services you wish to expose via your tunnel server. It is also possible to turn off Let's Encrypt support and use your own reverse proxy such as Caddy or Nginx. export DOMAIN = \"example.com\" inlets-pro http server \\ --auto-tls \\ --control-port 8123 \\ --auto-tls-san 192 .168.0.10 \\ --letsencrypt-domain subdomain1. $DOMAIN \\ --letsencrypt-domain subdomain2. $DOMAIN \\ --letsencrypt-email contact@ $DOMAIN \\ --letsencrypt-issuer staging --token $TOKEN Notice that --letsencrypt-domain can be provided more than one, for each of your subdomains. We are also defaulting to the \"staging\" provider for TLS certificates which allows us to obtain a large number of certificates for experimentation purposes only. The default value, if this field is left off is prod as you will see by running inlets-pro http server --help . Now the following will happen: The tunnel server will start up and listen to TCP traffic on port 80 and 443. The server will try to resolve each of your domains passed via --letsencrypt-domain . Then once each resolves, Let's Encrypt will be contacted for a HTTP01 ACME challenge. Once the certificates are obtained, the server will start serving the HTTPS traffic. Now you can connect your client running on another machine. Of course you can tunnel whatever HTTP service you like, if you already have one. Inlets has a built-in HTTP server that we can run on our local / private machine to share files with others. Let's use that as our example: mkdir -p /tmp/share echo \"Welcome to my filesharing service.\" > /tmp/share/welcome.txt inlets-pro fileserver \\ --allow-browsing \\ --webroot /tmp/share/ --port 8080 Next let's expose that local service running on localhost:8080 via the tunnel server: export TOKEN = \"\" # Obtain this from your server export SERVER_IP = \"\" # Your server's IP export DOMAIN = \"example.com\" inlets-pro http client \\ --url wss:// $SERVER_IP :8123 \\ --token $TOKEN \\ --upstream http://localhost:8080/ If you set up your server for more than one sub-domain then you can specify a domain for each local service such as: --upstream subdomain1. $DOMAIN = http://localhost:8080/,subdomain2. $DOMAIN = http://localhost:3000/ Now that your client is connected, you can access the HTTP fileserver we set up earlier via the public DNS name: curl -k -v https://subdomain1.$DOMAIN/welcome.txt Now that you can see everything working, with a staging certificate, you can run the server command again and switch out the --letsencrypt-issuer staging flag for --letsencrypt-issuer prod . Wrapping up \u00b6 You have now installed an inlets HTTP tunnel server to a machine by hand. The same can be achieved by running the inletsctl tool, which does all of this automatically on a number of cloud providers. Can I connect more than one client to the same server? Yes, and each can connect difference services. So client 1 exposes subdomain1. DOMAIN and client 2 exposes subdomain2. DOMAIN and client 2 exposes subdomain2. DOMAIN. Alternatively, you can have multiple clients exposing the same domain, for high availability. How do I keep the inlets server process running? You can run it in the background, by using a systemd unit file. You can generate these via the inlets-pro http server --generate=systemd command. How do I keep the inlets client process running? Do the same as for a server, but use the inlets-pro http client --generate=systemd command. What else can I do with my server? Browse the available options for the tunnel servers with the inlets-pro http server --help command.","title":"Manual http server"},{"location":"tutorial/manual-http-server/#setting-up-a-http-tunnel-server-manually","text":"In this tutorial we will set up an inlets HTTP tunnel server to serve a local website over HTTPS using Let's Encrypt. The steps will be manual, but usually, we would use a provisioning tool like inletsctl to automate everything for us. This may be useful for understanding how the server binary works, and how to use it on existing servers that you may have. Or perhaps you want to run inlets across an internal or private network.","title":"Setting up a HTTP tunnel server manually"},{"location":"tutorial/manual-http-server/#pre-reqs","text":"A Linux server, Windows and MacOS are also supported The inlets-pro binary at /usr/local/bin/ Access to a DNS control plane for a domain you control","title":"Pre-reqs"},{"location":"tutorial/manual-http-server/#run-the-server","text":"For this example, your tunnel server should be accessible from the Internet. The tunnel client will connect to it and then expose one or more local websites so that you can access them remotely. Create a DNS A record for the subdomain or subdomains you want to use, and have each of them point to the public IP address of the server you have provisioned. These short have a short TTL such as 60s to avoid waiting too long for DNS to propagate throughout the Internet. You can increase this value to a higher number later. First generate an authentication token that the client will use to log in: TOKEN = \" $( head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1 ) \" We'll use the built-in support for Let's Encrypt to get a valid HTTPS certificate for any services you wish to expose via your tunnel server. It is also possible to turn off Let's Encrypt support and use your own reverse proxy such as Caddy or Nginx. export DOMAIN = \"example.com\" inlets-pro http server \\ --auto-tls \\ --control-port 8123 \\ --auto-tls-san 192 .168.0.10 \\ --letsencrypt-domain subdomain1. $DOMAIN \\ --letsencrypt-domain subdomain2. $DOMAIN \\ --letsencrypt-email contact@ $DOMAIN \\ --letsencrypt-issuer staging --token $TOKEN Notice that --letsencrypt-domain can be provided more than one, for each of your subdomains. We are also defaulting to the \"staging\" provider for TLS certificates which allows us to obtain a large number of certificates for experimentation purposes only. The default value, if this field is left off is prod as you will see by running inlets-pro http server --help . Now the following will happen: The tunnel server will start up and listen to TCP traffic on port 80 and 443. The server will try to resolve each of your domains passed via --letsencrypt-domain . Then once each resolves, Let's Encrypt will be contacted for a HTTP01 ACME challenge. Once the certificates are obtained, the server will start serving the HTTPS traffic. Now you can connect your client running on another machine. Of course you can tunnel whatever HTTP service you like, if you already have one. Inlets has a built-in HTTP server that we can run on our local / private machine to share files with others. Let's use that as our example: mkdir -p /tmp/share echo \"Welcome to my filesharing service.\" > /tmp/share/welcome.txt inlets-pro fileserver \\ --allow-browsing \\ --webroot /tmp/share/ --port 8080 Next let's expose that local service running on localhost:8080 via the tunnel server: export TOKEN = \"\" # Obtain this from your server export SERVER_IP = \"\" # Your server's IP export DOMAIN = \"example.com\" inlets-pro http client \\ --url wss:// $SERVER_IP :8123 \\ --token $TOKEN \\ --upstream http://localhost:8080/ If you set up your server for more than one sub-domain then you can specify a domain for each local service such as: --upstream subdomain1. $DOMAIN = http://localhost:8080/,subdomain2. $DOMAIN = http://localhost:3000/ Now that your client is connected, you can access the HTTP fileserver we set up earlier via the public DNS name: curl -k -v https://subdomain1.$DOMAIN/welcome.txt Now that you can see everything working, with a staging certificate, you can run the server command again and switch out the --letsencrypt-issuer staging flag for --letsencrypt-issuer prod .","title":"Run the server"},{"location":"tutorial/manual-http-server/#wrapping-up","text":"You have now installed an inlets HTTP tunnel server to a machine by hand. The same can be achieved by running the inletsctl tool, which does all of this automatically on a number of cloud providers. Can I connect more than one client to the same server? Yes, and each can connect difference services. So client 1 exposes subdomain1. DOMAIN and client 2 exposes subdomain2. DOMAIN and client 2 exposes subdomain2. DOMAIN. Alternatively, you can have multiple clients exposing the same domain, for high availability. How do I keep the inlets server process running? You can run it in the background, by using a systemd unit file. You can generate these via the inlets-pro http server --generate=systemd command. How do I keep the inlets client process running? Do the same as for a server, but use the inlets-pro http client --generate=systemd command. What else can I do with my server? Browse the available options for the tunnel servers with the inlets-pro http server --help command.","title":"Wrapping up"},{"location":"tutorial/postgresql-tcp-tunnel/","text":"Tutorial: Tunnel a private Postgresql database \u00b6 In this tutorial we will tunnel Postgresql over inlets PRO to a remote machine. From there you can expose it to the Internet, or bind it to the local network for private VPN-like access. You can subscribe to inlets for personal or commercial use via Gumroad Setup your exit node \u00b6 Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl : inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --pro Note the --url and TOKEN given to you in this step. Run Postgresql on your private server \u00b6 We can run a Postgresql instance using Docker: head -c 16 /dev/urandom | shasum 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" docker run --rm --name postgres -p 5432 :5432 -e POSTGRES_PASSWORD = 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 -ti postgres:latest Connect the inlets PRO client \u00b6 Fill in the below with the outputs you received from inletsctl create . Note that UPSTREAM=\"localhost\" can be changed to point at a host or IP address accessible from your client. The choice of localhost is suitable when you are running Postgresql in Docker on the same computer as the inlets PRO client. The client will look for your license in $HOME/.inlets/LICENSE , but you can also use the --license/--license-file flag if you wish. export EXIT_IP = \"134.209.21.155\" export TCP_PORTS = \"5432\" export LICENSE_FILE = \" $HOME /LICENSE.txt\" export TOKEN = \"KXJ5Iq1Z5Cc8GjFXdXJrqNhUzoScXnZXOSRKeh8x3f6tdGq1ijdENWQ2IfzdCg4U\" export UPSTREAM = \"localhost\" inlets-pro client --connect \"wss:// $EXIT_IP :8123/connect\" \\ --token \" $TOKEN \" \\ --upstream $UPSTREAM \\ --ports $TCP_PORTS Connect to your private Postgresql server from the Internet \u00b6 You can run this command from anywhere, since your exit-server has a public IP: export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" export EXIT_IP = \"209.97.141.140\" docker run -it -e PGPORT = 5432 -e PGPASSWORD = $PASSWORD --rm postgres:latest psql -U postgres -h $EXIT_IP Try a command such as CREATE database or \\dt . Treat the database as private - like a VPN \u00b6 A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels If you would like to keep the database service and port private, you can run the exit-server as a Pod in a Kubernetes cluster, or add an iptables rule to block access from external IPs. Log into your exit-server and update /etc/systemd/system/inlets-pro.service To listen on loopback, add: --listen-data=127.0.0.1: To listen on a private adapter such as 10.1.0.10 , add: --listen-data=10.1.0.10: Restart the service, and you'll now find that the database port 5432 can only be accessed from within the network you specified in --listen-data Other databases such as Cassandra, MongoDB and Mysql/MariaDB also work exactly the same. Just change the port from 5432 to the port of your database.","title":"Tutorial: Tunnel a private Postgresql database"},{"location":"tutorial/postgresql-tcp-tunnel/#tutorial-tunnel-a-private-postgresql-database","text":"In this tutorial we will tunnel Postgresql over inlets PRO to a remote machine. From there you can expose it to the Internet, or bind it to the local network for private VPN-like access. You can subscribe to inlets for personal or commercial use via Gumroad","title":"Tutorial: Tunnel a private Postgresql database"},{"location":"tutorial/postgresql-tcp-tunnel/#setup-your-exit-node","text":"Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl : inletsctl create \\ --provider digitalocean \\ --region lon1 \\ --pro Note the --url and TOKEN given to you in this step.","title":"Setup your exit node"},{"location":"tutorial/postgresql-tcp-tunnel/#run-postgresql-on-your-private-server","text":"We can run a Postgresql instance using Docker: head -c 16 /dev/urandom | shasum 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" docker run --rm --name postgres -p 5432 :5432 -e POSTGRES_PASSWORD = 8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 -ti postgres:latest","title":"Run Postgresql on your private server"},{"location":"tutorial/postgresql-tcp-tunnel/#connect-the-inlets-pro-client","text":"Fill in the below with the outputs you received from inletsctl create . Note that UPSTREAM=\"localhost\" can be changed to point at a host or IP address accessible from your client. The choice of localhost is suitable when you are running Postgresql in Docker on the same computer as the inlets PRO client. The client will look for your license in $HOME/.inlets/LICENSE , but you can also use the --license/--license-file flag if you wish. export EXIT_IP = \"134.209.21.155\" export TCP_PORTS = \"5432\" export LICENSE_FILE = \" $HOME /LICENSE.txt\" export TOKEN = \"KXJ5Iq1Z5Cc8GjFXdXJrqNhUzoScXnZXOSRKeh8x3f6tdGq1ijdENWQ2IfzdCg4U\" export UPSTREAM = \"localhost\" inlets-pro client --connect \"wss:// $EXIT_IP :8123/connect\" \\ --token \" $TOKEN \" \\ --upstream $UPSTREAM \\ --ports $TCP_PORTS","title":"Connect the inlets PRO client"},{"location":"tutorial/postgresql-tcp-tunnel/#connect-to-your-private-postgresql-server-from-the-internet","text":"You can run this command from anywhere, since your exit-server has a public IP: export PASSWORD = \"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\" export EXIT_IP = \"209.97.141.140\" docker run -it -e PGPORT = 5432 -e PGPASSWORD = $PASSWORD --rm postgres:latest psql -U postgres -h $EXIT_IP Try a command such as CREATE database or \\dt .","title":"Connect to your private Postgresql server from the Internet"},{"location":"tutorial/postgresql-tcp-tunnel/#treat-the-database-as-private-like-a-vpn","text":"A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels If you would like to keep the database service and port private, you can run the exit-server as a Pod in a Kubernetes cluster, or add an iptables rule to block access from external IPs. Log into your exit-server and update /etc/systemd/system/inlets-pro.service To listen on loopback, add: --listen-data=127.0.0.1: To listen on a private adapter such as 10.1.0.10 , add: --listen-data=10.1.0.10: Restart the service, and you'll now find that the database port 5432 can only be accessed from within the network you specified in --listen-data Other databases such as Cassandra, MongoDB and Mysql/MariaDB also work exactly the same. Just change the port from 5432 to the port of your database.","title":"Treat the database as private - like a VPN"},{"location":"tutorial/ssh-tcp-tunnel/","text":"Tutorial: Expose a private SSH server over a TCP tunnel \u00b6 In this tutorial we will use inlets-pro to access your computer behind NAT or a firewall. We'll do this by tunnelling SSH over inlets-pro, and clients will connect to your exit-server. Scenario: You want to allow SSH access to a computer that doesn't have a public IP, is inside a private network or behind a firewall. A common scenario is connecting to a Raspberry Pi on a home network or a home-lab. You can subscribe to inlets for personal or commercial use via Gumroad Setup your tunnel server with inletsctl \u00b6 For this tutorial you will need to have an account and API key with one of the supported providers , or you can create an exit-server manually and install inlets PRO there yourself. For this tutorial, the DigitalOcean provider will be used . You can get free credits on DigitalOcean with this link . Create an API key in the DigitalOcean dashboard with Read and Write permissions, and download it to a file called do-access-token in your home directory. You need to know the IP of the machine you to connect to on your local network, for instance 192.168.0.35 or 127.0.0.1 if you are running inlets PRO on the same host as SSH. You can use the inletsctl utility to provision exit-servers with inlets PRO preinstalled, it can also download the inlets-pro CLI. curl -sLSf https://inletsctl.inlets.dev | sh sudo mv inletsctl /usr/local/bin/ sudo inletsctl download If you already have inletsctl installed, then make sure you update it with inletsctl update . Create an tunnel server \u00b6 A) Automate your tunnel server \u00b6 The inletsctl tool can create a tunnel server for you in the region and cloud of your choice. inletsctl create \\ --provider digitalocean \\ --access-token-file ~/do-access-token \\ --region lon1 Run inletsctl create --help to see all the options. After the machine has been created, inletsctl will output a sample command for the inlets-pro client command: inlets-pro client --url \"wss://206.189.114.179:8123/connect\" \\ --token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\" Don't run this command, but note down the --url and --token parameters for later B) Manual setup of your tunnel server \u00b6 Use B) if you want to provision your virtual machine manually, or if you already have a host from another provider. Log in to your remote tunnel server with ssh and obtain the binary using inletsctl : curl -sLSf https://inletsctl.inlets.dev | sh sudo mv inletsctl /usr/local/bin/ sudo inletsctl download Find your public IP: export IP = $( curl -s ifconfig.co ) Confirm the IP with echo $IP and save it, you need it for the client Get an auth token and save it for later to use with the client export TOKEN = \" $( head -c 16 /dev/urandom | shasum | cut -d '-' -f1 ) \" echo $TOKEN Start the server: inlets-pro server \\ --auto-tls \\ --auto-tls-san $IP \\ --token $TOKEN If running the inlets client on the same host as SSH, you can simply set PROXY_TO_HERE to localhost . Or if you are running SSH on a different computer to the inlets client, then you can specify a DNS entry or an IP address like 192.168.0.15 . If using this manual approach to install inlets PRO, you should create a systemd unit file. The easiest option is to run the server with the --generate=systemd flag, which will generate a systemd unit file to stdout. You can then copy the output to /etc/systemd/system/inlets-pro.service and enable it with systemctl enable inlets-pro . Configure the private SSH server's listening port \u00b6 It's very likely (almost certain) that your exit server will already be listening for traffic on the standard ssh port 22 . Therefore you will need to configure your internal server to use an additional TCP port such as 2222 . Once configured, you'll still be able to connect to the internal server on port 22, but to connect via the tunnel, you'll use port 2222 Add the following to /etc/ssh/sshd_config : Port 22 Port 2222 For (optional) additional security, you could also disable password authentication, but make sure that you have inserted your SSH key to the internal server with ssh-copy-id user@ip before reloading the SSH service. PasswordAuthentication no Now need to reload the service so these changes take effect sudo systemctl daemon-reload sudo systemctl restart sshd Check that you can still connect on the internal IP on port 22, and the new port 2222. Use the -p flag to specify the SSH port: export IP = \"192.168.0.35\" ssh -p 22 $IP \"uptime\" ssh -p 2222 $IP \"uptime\" Start the inlets PRO client \u00b6 First download the inlets-pro client onto the private SSH server: sudo inletsctl download Use the command from earlier to start the client on the server: export IP = \"206.189.114.179\" export TCP_PORTS = \"2222\" export LICENSE_FILE = \" $HOME /LICENSE.txt\" export UPSTREAM = \"localhost\" inlets-pro client --url \"wss:// $IP :8123/connect\" \\ --token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\" \\ --license-file \" $LICENSE_FILE \" \\ --upstream \" $UPSTREAM \" \\ --ports $TCP_PORTS The localhost value will be used for --upstream because the tunnel client is running on the same machine as the SSH service. However, you could run the client on another machine within the network, and then change the flag to point to the private SSH server's IP. Try it out \u00b6 Verify the installation by trying to SSH to the public IP, using port 2222 . ssh -p 2222 user@206.189.114.179 You should now have access to your server via SSH over the internet with the IP of the exit server. You can also use other compatible tools like sftp , scp and rsync , just make sure that you set the appropriate port flag. The port flag for sftp is -P rather than -p . Wrapping up \u00b6 The principles in this tutorial can be adapted for other protocols that run over TCP such as MongoDB or PostgreSQL, just adapt the port number as required. Quick-start: Tunnel a private database over inlets PRO Purchase inlets for personal or commercial use","title":"Tutorial: Expose a private SSH server over a TCP tunnel"},{"location":"tutorial/ssh-tcp-tunnel/#tutorial-expose-a-private-ssh-server-over-a-tcp-tunnel","text":"In this tutorial we will use inlets-pro to access your computer behind NAT or a firewall. We'll do this by tunnelling SSH over inlets-pro, and clients will connect to your exit-server. Scenario: You want to allow SSH access to a computer that doesn't have a public IP, is inside a private network or behind a firewall. A common scenario is connecting to a Raspberry Pi on a home network or a home-lab. You can subscribe to inlets for personal or commercial use via Gumroad","title":"Tutorial: Expose a private SSH server over a TCP tunnel"},{"location":"tutorial/ssh-tcp-tunnel/#setup-your-tunnel-server-with-inletsctl","text":"For this tutorial you will need to have an account and API key with one of the supported providers , or you can create an exit-server manually and install inlets PRO there yourself. For this tutorial, the DigitalOcean provider will be used . You can get free credits on DigitalOcean with this link . Create an API key in the DigitalOcean dashboard with Read and Write permissions, and download it to a file called do-access-token in your home directory. You need to know the IP of the machine you to connect to on your local network, for instance 192.168.0.35 or 127.0.0.1 if you are running inlets PRO on the same host as SSH. You can use the inletsctl utility to provision exit-servers with inlets PRO preinstalled, it can also download the inlets-pro CLI. curl -sLSf https://inletsctl.inlets.dev | sh sudo mv inletsctl /usr/local/bin/ sudo inletsctl download If you already have inletsctl installed, then make sure you update it with inletsctl update .","title":"Setup your tunnel server with inletsctl"},{"location":"tutorial/ssh-tcp-tunnel/#create-an-tunnel-server","text":"","title":"Create an tunnel server"},{"location":"tutorial/ssh-tcp-tunnel/#a-automate-your-tunnel-server","text":"The inletsctl tool can create a tunnel server for you in the region and cloud of your choice. inletsctl create \\ --provider digitalocean \\ --access-token-file ~/do-access-token \\ --region lon1 Run inletsctl create --help to see all the options. After the machine has been created, inletsctl will output a sample command for the inlets-pro client command: inlets-pro client --url \"wss://206.189.114.179:8123/connect\" \\ --token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\" Don't run this command, but note down the --url and --token parameters for later","title":"A) Automate your tunnel server"},{"location":"tutorial/ssh-tcp-tunnel/#b-manual-setup-of-your-tunnel-server","text":"Use B) if you want to provision your virtual machine manually, or if you already have a host from another provider. Log in to your remote tunnel server with ssh and obtain the binary using inletsctl : curl -sLSf https://inletsctl.inlets.dev | sh sudo mv inletsctl /usr/local/bin/ sudo inletsctl download Find your public IP: export IP = $( curl -s ifconfig.co ) Confirm the IP with echo $IP and save it, you need it for the client Get an auth token and save it for later to use with the client export TOKEN = \" $( head -c 16 /dev/urandom | shasum | cut -d '-' -f1 ) \" echo $TOKEN Start the server: inlets-pro server \\ --auto-tls \\ --auto-tls-san $IP \\ --token $TOKEN If running the inlets client on the same host as SSH, you can simply set PROXY_TO_HERE to localhost . Or if you are running SSH on a different computer to the inlets client, then you can specify a DNS entry or an IP address like 192.168.0.15 . If using this manual approach to install inlets PRO, you should create a systemd unit file. The easiest option is to run the server with the --generate=systemd flag, which will generate a systemd unit file to stdout. You can then copy the output to /etc/systemd/system/inlets-pro.service and enable it with systemctl enable inlets-pro .","title":"B) Manual setup of your tunnel server"},{"location":"tutorial/ssh-tcp-tunnel/#configure-the-private-ssh-servers-listening-port","text":"It's very likely (almost certain) that your exit server will already be listening for traffic on the standard ssh port 22 . Therefore you will need to configure your internal server to use an additional TCP port such as 2222 . Once configured, you'll still be able to connect to the internal server on port 22, but to connect via the tunnel, you'll use port 2222 Add the following to /etc/ssh/sshd_config : Port 22 Port 2222 For (optional) additional security, you could also disable password authentication, but make sure that you have inserted your SSH key to the internal server with ssh-copy-id user@ip before reloading the SSH service. PasswordAuthentication no Now need to reload the service so these changes take effect sudo systemctl daemon-reload sudo systemctl restart sshd Check that you can still connect on the internal IP on port 22, and the new port 2222. Use the -p flag to specify the SSH port: export IP = \"192.168.0.35\" ssh -p 22 $IP \"uptime\" ssh -p 2222 $IP \"uptime\"","title":"Configure the private SSH server's listening port"},{"location":"tutorial/ssh-tcp-tunnel/#start-the-inlets-pro-client","text":"First download the inlets-pro client onto the private SSH server: sudo inletsctl download Use the command from earlier to start the client on the server: export IP = \"206.189.114.179\" export TCP_PORTS = \"2222\" export LICENSE_FILE = \" $HOME /LICENSE.txt\" export UPSTREAM = \"localhost\" inlets-pro client --url \"wss:// $IP :8123/connect\" \\ --token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\" \\ --license-file \" $LICENSE_FILE \" \\ --upstream \" $UPSTREAM \" \\ --ports $TCP_PORTS The localhost value will be used for --upstream because the tunnel client is running on the same machine as the SSH service. However, you could run the client on another machine within the network, and then change the flag to point to the private SSH server's IP.","title":"Start the inlets PRO client"},{"location":"tutorial/ssh-tcp-tunnel/#try-it-out","text":"Verify the installation by trying to SSH to the public IP, using port 2222 . ssh -p 2222 user@206.189.114.179 You should now have access to your server via SSH over the internet with the IP of the exit server. You can also use other compatible tools like sftp , scp and rsync , just make sure that you set the appropriate port flag. The port flag for sftp is -P rather than -p .","title":"Try it out"},{"location":"tutorial/ssh-tcp-tunnel/#wrapping-up","text":"The principles in this tutorial can be adapted for other protocols that run over TCP such as MongoDB or PostgreSQL, just adapt the port number as required. Quick-start: Tunnel a private database over inlets PRO Purchase inlets for personal or commercial use","title":"Wrapping up"}]}